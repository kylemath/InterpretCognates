{"role":"user","message":{"content":[{"type":"text","text":"<user_query>\nYou are implementing scientific analysis functions for the InterpretCognates project. Edit the file `/Users/kylemathewson/InterpretCognates/backend/app/modeling.py` to ADD the following new functions. DO NOT modify or remove any existing functions. Add the new functions AFTER all existing functions.\n\nThe existing file already has these imports and globals:\n```python\nimport os\nimport threading\nfrom typing import Any\nimport numpy as np\nimport torch\nfrom sklearn.decomposition import PCA\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nMODEL_NAME = os.getenv(\"NLLB_MODEL\", \"facebook/nllb-200-distilled-600M\")\n_MODEL = None\n_TOKENIZER = None\n_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n_LOCK = threading.Lock()\n```\n\nExisting helper functions: `_ensure_model_loaded()`, `_pool_hidden(last_hidden, attention_mask)`, `_clean_tokens(tokens)`, `_token_alignment_fallback(...)`.\nExisting public functions: `translate_text()`, `embed_text()`, `sentence_similarity_matrix()`, `project_embeddings()`, `cross_attention_map()`.\n\nADD these 8 new functions at the end of the file:\n\n### 1. `abtt_correct(vectors, k=3)` — Task 1: ABTT Isotropy Correction\nSubtracts global mean and removes top-k principal components to fix anisotropy bias in cosine similarity.\n```python\ndef abtt_correct(vectors: list[np.ndarray], k: int = 3) -> list[np.ndarray]:\n    array = np.array(vectors)\n    array = array - array.mean(axis=0)\n    pca = PCA(n_components=k)\n    pca.fit(array)\n    for component in pca.components_:\n        array = array - (array @ component[:, None]) * component[None, :]\n    return [array[i] for i in range(len(vectors))]\n```\n\n### 2. `sentence_similarity_matrix_corrected(vectors, k=3)` — Task 1 companion\n```python\ndef sentence_similarity_matrix_corrected(vectors: list[np.ndarray], k: int = 3) -> list[list[float]]:\n    corrected = abtt_correct(vectors, k=k)\n    return sentence_similarity_matrix(corrected)\n```\n\n### 3. `project_embeddings_mean_centered(vectors, labels, language_ids)` — Task 2: Mean-Centering for PCA\nReturns both raw and centered PCA projections. The centered version subtracts per-language mean vectors before PCA.\n```python\ndef project_embeddings_mean_centered(\n    vectors: list[np.ndarray], labels: list[str], language_ids: list[str]\n) -> dict[str, Any]:\n    array = np.array(vectors)\n    lang_set = sorted(set(language_ids))\n    lang_means = {}\n    for lang in lang_set:\n        indices = [i for i, l in enumerate(language_ids) if l == lang]\n        lang_means[lang] = array[indices].mean(axis=0)\n    centered = np.array([array[i] - lang_means[language_ids[i]] for i in range(len(vectors))])\n\n    n = len(vectors)\n    n_components = min(3, n, array.shape[1])\n    raw_proj = PCA(n_components=n_components).fit_transform(array)\n    centered_proj = PCA(n_components=n_components).fit_transform(centered)\n\n    def _to_points(proj, lbls):\n        points = []\n        for i, label in enumerate(lbls):\n            points.append({\n                \"label\": label,\n                \"x\": float(proj[i, 0]),\n                \"y\": float(proj[i, 1]),\n                \"z\": float(proj[i, 2]) if n_components >= 3 else 0.0,\n            })\n        return points\n\n    return {\"raw\": _to_points(raw_proj, labels), \"centered\": _to_points(centered_proj, labels)}\n```\n\n### 4. `embed_text_all_layers(text, lang)` — Task 3: Per-Layer Embedding Extraction\nReturns a list of 13 mean-pooled vectors (embedding layer + 12 encoder layers) by passing output_hidden_states=True.\n```python\ndef embed_text_all_layers(text: str, lang: str) -> list[np.ndarray]:\n    model, tokenizer = _ensure_model_loaded()\n    tokenizer.src_lang = lang\n    encoded = tokenizer(text, return_tensors=\"pt\").to(_DEVICE)\n    with torch.no_grad():\n        out = model.model.encoder(\n            input_ids=encoded[\"input_ids\"],\n            attention_mask=encoded[\"attention_mask\"],\n            output_hidden_states=True,\n            return_dict=True,\n        )\n    return [\n        _pool_hidden(h, encoded[\"attention_mask\"]).squeeze(0).cpu().numpy()\n        for h in out.hidden_states\n    ]\n```\n\n### 5. `embed_text_batch(texts, langs)` — Task 4: Batch Embedding\nEfficiently embeds multiple texts, grouping by language for tokenizer efficiency.\n```python\ndef embed_text_batch(texts: list[str], langs: list[str]) -> list[np.ndarray]:\n    model, tokenizer = _ensure_model_loaded()\n    results: list[tuple[int, np.ndarray]] = []\n    indexed = list(enumerate(zip(texts, langs)))\n    indexed.sort(key=lambda x: x[1][1])\n\n    for idx, (text, lang) in indexed:\n        tokenizer.src_lang = lang\n        encoded = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(_DEVICE)\n        with torch.no_grad():\n            out = model.model.encoder(**encoded, return_dict=True)\n        vec = _pool_hidden(out.last_hidden_state, encoded[\"attention_mask\"]).squeeze(0).cpu().numpy()\n        results.append((idx, vec))\n\n    results.sort(key=lambda x: x[0])\n    return [vec for _, vec in results]\n```\n\n### 6. `cross_attention_map_per_head(source_text, source_lang, target_text, target_lang)` — Task 5: Per-Head Decomposition\nReturns the full (num_layers, num_heads, tgt_len, src_len) tensor with per-head confidence scores.\n```python\ndef cross_attention_map_per_head(\n    source_text: str,\n    source_lang: str,\n    target_text: str,\n    target_lang: str,\n) -> dict[str, Any]:\n    model, tokenizer = _ensure_model_loaded()\n    tokenizer.src_lang = source_lang\n    encoded_source = tokenizer(source_text, return_tensors=\"pt\").to(_DEVICE)\n    encoded_target = tokenizer(target_text, return_tensors=\"pt\").to(_DEVICE)\n    decoder_input_ids = encoded_target[\"input_ids\"][:, :-1]\n\n    if decoder_input_ids.shape[1] == 0:\n        return _token_alignment_fallback(\n            model, tokenizer, source_text, source_lang, target_text, target_lang\n        )\n\n    with torch.no_grad():\n        outputs = model(\n            input_ids=encoded_source[\"input_ids\"],\n            attention_mask=encoded_source[\"attention_mask\"],\n            decoder_input_ids=decoder_input_ids,\n            output_attentions=True,\n            return_dict=True,\n        )\n\n    cross_layers = [layer for layer in (outputs.cross_attentions or []) if layer is not None]\n    if not cross_layers:\n        return _token_alignment_fallback(\n            model, tokenizer, source_text, source_lang, target_text, target_lang\n        )\n\n    per_head_layers = []\n    for layer_idx, layer_attn in enumerate(cross_layers):\n        heads = layer_attn[0]  # (num_heads, tgt_len, src_len)\n        confidence = heads.max(dim=-1).values.mean(dim=-1)  # (num_heads,)\n        per_head_layers.append({\n            \"layer\": layer_idx,\n            \"head_confidence\": confidence.cpu().numpy().tolist(),\n            \"attention\": heads.cpu().numpy().tolist(),\n        })\n\n    source_tokens = tokenizer.convert_ids_to_tokens(encoded_source[\"input_ids\"][0])\n    target_tokens = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n    return {\n        \"source_tokens\": _clean_tokens(source_tokens),\n        \"target_tokens\": _clean_tokens(target_tokens),\n        \"per_head_layers\": per_head_layers,\n    }\n```\n\n### 7. `attention_entropy(attn_values)` — Task 7: Cognate vs Non-Cognate Entropy\nComputes the mean Shannon entropy of a cross-attention matrix. Lower entropy = more focused (diagonal) attention.\n```python\ndef attention_entropy(attn_values: list[list[float]]) -> float:\n    mat = np.array(attn_values)\n    mat = mat / np.clip(mat.sum(axis=1, keepdims=True), 1e-8, None)\n    H = -np.sum(mat * np.log(mat + 1e-10), axis=1)\n    return float(H.mean())\n```\n\n### 8. `neuron_activation_mask(text, lang, threshold_pct=0.9)` and `neuron_overlap_matrix(texts, langs, threshold_pct=0.9)` — Task 10: Lottery Ticket Test\nUses forward hooks to capture FFN activations and compute Jaccard similarity of active neuron sets.\n```python\ndef neuron_activation_mask(text: str, lang: str, threshold_pct: float = 0.9) -> np.ndarray:\n    model, tokenizer = _ensure_model_loaded()\n    tokenizer.src_lang = lang\n    encoded = tokenizer(text, return_tensors=\"pt\").to(_DEVICE)\n\n    activations = {}\n    def hook_fn(module, input, output):\n        activations[\"ffn\"] = output.detach()\n\n    last_ffn = model.model.encoder.layers[-1].fc2\n    handle = last_ffn.register_forward_hook(hook_fn)\n    try:\n        with torch.no_grad():\n            model.model.encoder(\n                input_ids=encoded[\"input_ids\"],\n                attention_mask=encoded[\"attention_mask\"],\n                return_dict=True,\n            )\n    finally:\n        handle.remove()\n\n    act = activations[\"ffn\"].squeeze(0)  # (seq_len, hidden_dim)\n    max_act = act.abs().max(dim=0).values.cpu().numpy()  # (hidden_dim,)\n    threshold = np.percentile(max_act, threshold_pct * 100)\n    mask = (max_act >= threshold).astype(np.int8)\n    return mask\n\ndef neuron_overlap_matrix(\n    texts: list[str], langs: list[str], threshold_pct: float = 0.9\n) -> list[list[float]]:\n    masks = [neuron_activation_mask(text, lang, threshold_pct) for text, lang in zip(texts, langs)]\n    n = len(masks)\n    matrix = [[0.0] * n for _ in range(n)]\n    for i in range(n):\n        for j in range(n):\n            intersection = np.sum(masks[i] & masks[j])\n            union = np.sum(masks[i] | masks[j])\n            matrix[i][j] = float(intersection / max(union, 1))\n    return matrix\n```\n\nIMPORTANT:\n- Read the existing file first before making changes\n- ADD all functions at the END of the file, after the existing `cross_attention_map` function\n- Do NOT modify any existing functions\n- Do NOT change any imports — the existing imports (numpy, PCA, torch, etc.) are sufficient for all new functions\n- Make sure indentation is consistent (4 spaces)\n- Return the list of function signatures you added when complete\n</user_query>"}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Let me read the existing file first, then add the new functions at the end."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Now I'll append all 8 new functions at the end of the file."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"All 8 new functions (9 function definitions) have been added at the end of the file with no linter errors. Here are the signatures added:\n\n1. `abtt_correct(vectors: list[np.ndarray], k: int = 3) -> list[np.ndarray]`\n2. `sentence_similarity_matrix_corrected(vectors: list[np.ndarray], k: int = 3) -> list[list[float]]`\n3. `project_embeddings_mean_centered(vectors: list[np.ndarray], labels: list[str], language_ids: list[str]) -> dict[str, Any]`\n4. `embed_text_all_layers(text: str, lang: str) -> list[np.ndarray]`\n5. `embed_text_batch(texts: list[str], langs: list[str]) -> list[np.ndarray]`\n6. `cross_attention_map_per_head(source_text: str, source_lang: str, target_text: str, target_lang: str) -> dict[str, Any]`\n7. `attention_entropy(attn_values: list[list[float]]) -> float`\n8. `neuron_activation_mask(text: str, lang: str, threshold_pct: float = 0.9) -> np.ndarray`\n9. `neuron_overlap_matrix(texts: list[str], langs: list[str], threshold_pct: float = 0.9) -> list[list[float]]`"}]}}