{"role":"user","message":{"content":[{"type":"text","text":"<user_query>\nYou need to update TWO files for the InterpretCognates project:\n\n## File 1: `/Users/kylemathewson/InterpretCognates/backend/requirements.txt`\n\nThe current content is:\n```\nfastapi\nnumpy\nscikit-learn\nsentencepiece\nsafetensors\ntorch\ntransformers\nuvicorn\n```\n\nAdd `scipy` as a new dependency (needed for benchmarks module). Update it to:\n```\nfastapi\nnumpy\nscikit-learn\nscipy\nsentencepiece\nsafetensors\ntorch\ntransformers\nuvicorn\n```\n\n## File 2: `/Users/kylemathewson/InterpretCognates/backend/app/main.py`\n\nRead the current file first. The current file has a single `POST /api/analyze` endpoint and a `GET /` index endpoint. You need to:\n\n1. Add imports for the new functions from modeling.py and benchmarks.py\n2. Update the existing `/api/analyze` endpoint to support isotropy correction and mean-centering\n3. Add 7 new API endpoints\n\nHere is the COMPLETE new version of main.py to write:\n\n```python\nfrom pathlib import Path\n\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import FileResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom pydantic import BaseModel, Field\n\nfrom .modeling import (\n    attention_entropy,\n    cross_attention_map,\n    cross_attention_map_per_head,\n    embed_text,\n    embed_text_all_layers,\n    embed_text_batch,\n    neuron_activation_mask,\n    neuron_overlap_matrix,\n    project_embeddings,\n    project_embeddings_mean_centered,\n    sentence_similarity_matrix,\n    sentence_similarity_matrix_corrected,\n)\nfrom .benchmarks import (\n    colexification_test,\n    compute_swadesh_embedding_distances,\n    cosine_distance_matrix,\n    load_color_terms,\n    load_swadesh_corpus,\n    mantel_test,\n    swadesh_convergence_ranking,\n)\n\nimport numpy as np\n\n# ---------------------------------------------------------------------------\n# Request models\n# ---------------------------------------------------------------------------\n\nclass AnalyzeRequest(BaseModel):\n    concept: str = Field(..., min_length=1, max_length=80)\n    source_lang: str = \"eng_Latn\"\n    target_langs: list[str] = Field(default_factory=lambda: [\"spa_Latn\", \"fra_Latn\", \"deu_Latn\"])\n    context_template: str = \"I saw a {word} near the river.\"\n    isotropy_corrected: bool = False\n\nclass LayerEmbedRequest(BaseModel):\n    text: str = Field(..., min_length=1)\n    lang: str = \"eng_Latn\"\n\nclass PerHeadAttentionRequest(BaseModel):\n    source_text: str = Field(..., min_length=1)\n    source_lang: str = \"eng_Latn\"\n    target_text: str = Field(..., min_length=1)\n    target_lang: str = \"spa_Latn\"\n\nclass CognateEntropyRequest(BaseModel):\n    pairs: list[dict] = Field(\n        ...,\n        description=\"List of {source_text, source_lang, target_text, target_lang, category} dicts\",\n    )\n\nclass NeuronOverlapRequest(BaseModel):\n    texts: list[str] = Field(..., min_items=2)\n    langs: list[str] = Field(..., min_items=2)\n    threshold_pct: float = 0.9\n\n# ---------------------------------------------------------------------------\n# App setup\n# ---------------------------------------------------------------------------\n\napp = FastAPI(title=\"Interpret Cognates\")\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\nstatic_dir = Path(__file__).resolve().parent / \"static\"\napp.mount(\"/static\", StaticFiles(directory=static_dir), name=\"static\")\n\n@app.get(\"/\")\ndef index() -> FileResponse:\n    return FileResponse(static_dir / \"index.html\")\n\n# ---------------------------------------------------------------------------\n# Core analysis endpoint (updated with isotropy + mean-centering)\n# ---------------------------------------------------------------------------\n\n@app.post(\"/api/analyze\")\ndef analyze(payload: AnalyzeRequest) -> dict:\n    source_sentence = payload.context_template.format(word=payload.concept)\n\n    labels = [payload.source_lang]\n    language_ids = [payload.source_lang]\n    sentences = [source_sentence]\n    vectors = [embed_text(source_sentence, payload.source_lang)]\n    translations = []\n    attention_maps = []\n\n    for lang in payload.target_langs:\n        translated_sentence = translate_text(source_sentence, payload.source_lang, lang)\n        translations.append({\"lang\": lang, \"text\": translated_sentence})\n        labels.append(lang)\n        language_ids.append(lang)\n        sentences.append(translated_sentence)\n        vectors.append(embed_text(translated_sentence, lang))\n        attention = cross_attention_map(source_sentence, payload.source_lang, translated_sentence, lang)\n        attention_maps.append({\"lang\": lang, **attention})\n\n    points = project_embeddings(vectors, labels)\n\n    if payload.isotropy_corrected:\n        similarity = sentence_similarity_matrix_corrected(vectors)\n    else:\n        similarity = sentence_similarity_matrix(vectors)\n\n    mean_centered = project_embeddings_mean_centered(vectors, labels, language_ids)\n\n    return {\n        \"concept\": payload.concept,\n        \"source_sentence\": source_sentence,\n        \"labels\": labels,\n        \"language_ids\": language_ids,\n        \"sentences\": sentences,\n        \"translations\": translations,\n        \"embedding_points\": points,\n        \"embedding_points_centered\": mean_centered[\"centered\"],\n        \"similarity_matrix\": similarity,\n        \"attention_maps\": attention_maps,\n    }\n\n# ---------------------------------------------------------------------------\n# Per-layer embedding endpoint (Task 3)\n# ---------------------------------------------------------------------------\n\n@app.post(\"/api/embed/layers\")\ndef embed_layers(payload: LayerEmbedRequest) -> dict:\n    layer_vectors = embed_text_all_layers(payload.text, payload.lang)\n    return {\n        \"text\": payload.text,\n        \"lang\": payload.lang,\n        \"num_layers\": len(layer_vectors),\n        \"layer_embeddings\": [vec.tolist() for vec in layer_vectors],\n    }\n\n# ---------------------------------------------------------------------------\n# Per-head cross-attention endpoint (Task 5)\n# ---------------------------------------------------------------------------\n\n@app.post(\"/api/cross-attention/per-head\")\ndef cross_attention_per_head(payload: PerHeadAttentionRequest) -> dict:\n    result = cross_attention_map_per_head(\n        payload.source_text, payload.source_lang,\n        payload.target_text, payload.target_lang,\n    )\n    return result\n\n# ---------------------------------------------------------------------------\n# Swadesh experiment endpoint (Task 4)\n# ---------------------------------------------------------------------------\n\n@app.post(\"/api/experiment/swadesh\")\ndef experiment_swadesh() -> dict:\n    corpus = load_swadesh_corpus()\n    languages = [lang[\"code\"] for lang in corpus[\"languages\"]]\n    concepts = corpus[\"concepts\"]\n\n    concept_embeddings: dict[str, dict[str, np.ndarray]] = {}\n    all_texts = []\n    all_langs = []\n    all_keys = []\n\n    for concept_name, translations in concepts.items():\n        for lang_code in languages:\n            word = translations.get(lang_code)\n            if word:\n                all_texts.append(word)\n                all_langs.append(lang_code)\n                all_keys.append((concept_name, lang_code))\n\n    vectors = embed_text_batch(all_texts, all_langs)\n\n    for (concept_name, lang_code), vec in zip(all_keys, vectors):\n        if concept_name not in concept_embeddings:\n            concept_embeddings[concept_name] = {}\n        concept_embeddings[concept_name][lang_code] = vec\n\n    ranking = swadesh_convergence_ranking(concept_embeddings, languages)\n\n    return {\n        \"num_concepts\": len(concepts),\n        \"num_languages\": len(languages),\n        \"total_embeddings\": len(vectors),\n        \"convergence_ranking\": ranking,\n    }\n\n# ---------------------------------------------------------------------------\n# Phylogenetic correlation endpoint (Task 6)\n# ---------------------------------------------------------------------------\n\n@app.post(\"/api/experiment/phylogenetic\")\ndef experiment_phylogenetic() -> dict:\n    corpus = load_swadesh_corpus()\n    languages = [lang[\"code\"] for lang in corpus[\"languages\"]]\n    concepts = corpus[\"concepts\"]\n\n    concept_embeddings: dict[str, dict[str, np.ndarray]] = {}\n    all_texts = []\n    all_langs = []\n    all_keys = []\n\n    for concept_name, translations in concepts.items():\n        for lang_code in languages:\n            word = translations.get(lang_code)\n            if word:\n                all_texts.append(word)\n                all_langs.append(lang_code)\n                all_keys.append((concept_name, lang_code))\n\n    vectors = embed_text_batch(all_texts, all_langs)\n\n    for (concept_name, lang_code), vec in zip(all_keys, vectors):\n        if concept_name not in concept_embeddings:\n            concept_embeddings[concept_name] = {}\n        concept_embeddings[concept_name][lang_code] = vec\n\n    embedding_dist = compute_swadesh_embedding_distances(concept_embeddings, languages)\n\n    return {\n        \"num_languages\": len(languages),\n        \"languages\": languages,\n        \"embedding_distance_matrix\": embedding_dist.tolist(),\n        \"note\": \"To complete phylogenetic test, supply ASJP distance matrix and call /api/experiment/mantel\",\n    }\n\n# ---------------------------------------------------------------------------\n# Cognate entropy experiment endpoint (Task 7)\n# ---------------------------------------------------------------------------\n\n@app.post(\"/api/experiment/cognate-entropy\")\ndef experiment_cognate_entropy(payload: CognateEntropyRequest) -> dict:\n    results_by_category: dict[str, list[float]] = {}\n\n    for pair in payload.pairs:\n        attn = cross_attention_map(\n            pair[\"source_text\"], pair[\"source_lang\"],\n            pair[\"target_text\"], pair[\"target_lang\"],\n        )\n        entropy = attention_entropy(attn[\"values\"])\n        cat = pair.get(\"category\", \"unknown\")\n        if cat not in results_by_category:\n            results_by_category[cat] = []\n        results_by_category[cat].append(entropy)\n\n    summary = {}\n    for cat, entropies in results_by_category.items():\n        summary[cat] = {\n            \"mean_entropy\": float(np.mean(entropies)),\n            \"std_entropy\": float(np.std(entropies)),\n            \"count\": len(entropies),\n            \"entropies\": entropies,\n        }\n\n    return {\"category_summary\": summary}\n\n# ---------------------------------------------------------------------------\n# Berlin & Kay color circle endpoint (Task 8)\n# ---------------------------------------------------------------------------\n\n@app.post(\"/api/experiment/color-circle\")\ndef experiment_color_circle() -> dict:\n    color_data = load_color_terms()\n    languages = [lang[\"code\"] for lang in color_data[\"languages\"]]\n    colors = color_data[\"colors\"]\n\n    all_texts = []\n    all_langs = []\n    all_keys = []\n\n    for color_name, translations in colors.items():\n        for lang_code in languages:\n            word = translations.get(lang_code)\n            if word:\n                all_texts.append(word)\n                all_langs.append(lang_code)\n                all_keys.append((color_name, lang_code))\n\n    vectors = embed_text_batch(all_texts, all_langs)\n\n    color_vectors: dict[str, list[np.ndarray]] = {}\n    for (color_name, _), vec in zip(all_keys, vectors):\n        if color_name not in color_vectors:\n            color_vectors[color_name] = []\n        color_vectors[color_name].append(vec)\n\n    centroid_labels = []\n    centroid_vecs = []\n    for color_name, vecs in color_vectors.items():\n        centroid = np.mean(vecs, axis=0)\n        centroid_labels.append(color_name)\n        centroid_vecs.append(centroid)\n\n    points = project_embeddings(centroid_vecs, centroid_labels)\n\n    return {\n        \"num_colors\": len(colors),\n        \"num_languages\": len(languages),\n        \"centroids\": points,\n    }\n\n# ---------------------------------------------------------------------------\n# CLICS² colexification endpoint (Task 9)\n# ---------------------------------------------------------------------------\n\n@app.post(\"/api/experiment/colexification\")\ndef experiment_colexification() -> dict:\n    corpus = load_swadesh_corpus()\n    languages = [lang[\"code\"] for lang in corpus[\"languages\"]]\n    concepts = corpus[\"concepts\"]\n\n    concept_embeddings: dict[str, dict[str, np.ndarray]] = {}\n    all_texts = []\n    all_langs = []\n    all_keys = []\n\n    for concept_name, translations in concepts.items():\n        for lang_code in languages:\n            word = translations.get(lang_code)\n            if word:\n                all_texts.append(word)\n                all_langs.append(lang_code)\n                all_keys.append((concept_name, lang_code))\n\n    vectors = embed_text_batch(all_texts, all_langs)\n\n    for (concept_name, lang_code), vec in zip(all_keys, vectors):\n        if concept_name not in concept_embeddings:\n            concept_embeddings[concept_name] = {}\n        concept_embeddings[concept_name][lang_code] = vec\n\n    result = colexification_test(concept_embeddings, languages)\n    return result\n\n# ---------------------------------------------------------------------------\n# Neuron overlap endpoint (Task 10)\n# ---------------------------------------------------------------------------\n\n@app.post(\"/api/experiment/neuron-overlap\")\ndef experiment_neuron_overlap(payload: NeuronOverlapRequest) -> dict:\n    overlap = neuron_overlap_matrix(payload.texts, payload.langs, payload.threshold_pct)\n    return {\n        \"texts\": payload.texts,\n        \"langs\": payload.langs,\n        \"threshold_pct\": payload.threshold_pct,\n        \"overlap_matrix\": overlap,\n    }\n```\n\nIMPORTANT: The file above uses `translate_text` from modeling.py but it's NOT in the imports block. You need to add it. The complete import from modeling should include `translate_text` too. Here's the corrected import:\n\n```python\nfrom .modeling import (\n    attention_entropy,\n    cross_attention_map,\n    cross_attention_map_per_head,\n    embed_text,\n    embed_text_all_layers,\n    embed_text_batch,\n    neuron_activation_mask,\n    neuron_overlap_matrix,\n    project_embeddings,\n    project_embeddings_mean_centered,\n    sentence_similarity_matrix,\n    sentence_similarity_matrix_corrected,\n    translate_text,\n)\n```\n\nRead the existing main.py first, then replace it entirely with the new version above (including the translate_text fix in imports). Make sure the file is syntactically valid Python.\n\nReturn a list of all endpoints when done.\n</user_query>"}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Let me read the existing files first, then make the changes."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"I've read both files. Now I'll update them."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Let me verify both files and check for lint errors."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Both files updated successfully with no lint errors. Here's a summary of all endpoints in the updated `main.py`:\n\n| # | Method | Path | Description |\n|---|--------|------|-------------|\n| 1 | `GET` | `/` | Serves the static `index.html` page |\n| 2 | `POST` | `/api/analyze` | Core analysis — embeddings, similarity, attention maps (now with isotropy correction + mean-centering) |\n| 3 | `POST` | `/api/embed/layers` | Per-layer embeddings for a single text |\n| 4 | `POST` | `/api/cross-attention/per-head` | Per-head cross-attention between source/target |\n| 5 | `POST` | `/api/experiment/swadesh` | Swadesh list convergence ranking |\n| 6 | `POST` | `/api/experiment/phylogenetic` | Phylogenetic embedding distance matrix |\n| 7 | `POST` | `/api/experiment/cognate-entropy` | Cognate vs. non-cognate attention entropy |\n| 8 | `POST` | `/api/experiment/color-circle` | Berlin & Kay color term centroids |\n| 9 | `POST` | `/api/experiment/colexification` | CLICS² colexification test |\n| 10 | `POST` | `/api/experiment/neuron-overlap` | Neuron activation overlap matrix |"}]}}