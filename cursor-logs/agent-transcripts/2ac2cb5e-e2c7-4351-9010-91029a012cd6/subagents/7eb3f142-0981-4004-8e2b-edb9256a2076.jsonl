{"role":"user","message":{"content":[{"type":"text","text":"<user_query>\nCreate a new file `/Users/kylemathewson/InterpretCognates/backend/app/benchmarks.py` — a benchmarks module for the InterpretCognates project. This module implements Tasks 6 (Phylogenetic Mantel Test) and 9 (CLICS² Colexification Proximity Test) from the research roadmap.\n\nCreate the file with the following content:\n\n```python\n\"\"\"Benchmarks module for InterpretCognates.\n\nImplements external validation experiments:\n- Phylogenetic distance correlation (Mantel test against ASJP)\n- CLICS² colexification proximity test\n- Language code mapping utilities\n\"\"\"\n\nimport json\nfrom pathlib import Path\nfrom typing import Any\n\nimport numpy as np\nfrom scipy.stats import spearmanr, mannwhitneyu\n\nDATA_DIR = Path(__file__).resolve().parent / \"data\"\n\n# ---------------------------------------------------------------------------\n# Language code mapping: NLLB BCP-47 → ISO 639-3 → ASJP\n# Manually verified subset covering 40 major languages.\n# ---------------------------------------------------------------------------\n\nNLLB_TO_ISO = {\n    \"eng_Latn\": \"eng\", \"spa_Latn\": \"spa\", \"fra_Latn\": \"fra\", \"deu_Latn\": \"deu\",\n    \"ita_Latn\": \"ita\", \"por_Latn\": \"por\", \"rus_Cyrl\": \"rus\", \"pol_Latn\": \"pol\",\n    \"hin_Deva\": \"hin\", \"pes_Arab\": \"fas\", \"ell_Grek\": \"ell\", \"ron_Latn\": \"ron\",\n    \"nld_Latn\": \"nld\", \"swe_Latn\": \"swe\", \"ben_Beng\": \"ben\", \"zho_Hans\": \"cmn\",\n    \"jpn_Jpan\": \"jpn\", \"kor_Hang\": \"kor\", \"arb_Arab\": \"arb\", \"heb_Hebr\": \"heb\",\n    \"tur_Latn\": \"tur\", \"vie_Latn\": \"vie\", \"tha_Thai\": \"tha\", \"ind_Latn\": \"ind\",\n    \"tgl_Latn\": \"tgl\", \"swh_Latn\": \"swh\", \"yor_Latn\": \"yor\", \"hau_Latn\": \"hau\",\n    \"fin_Latn\": \"fin\", \"hun_Latn\": \"hun\", \"kat_Geor\": \"kat\", \"tam_Taml\": \"tam\",\n    \"tel_Telu\": \"tel\", \"mya_Mymr\": \"mya\", \"khm_Khmr\": \"khm\", \"amh_Ethi\": \"amh\",\n    \"eus_Latn\": \"eus\", \"kaz_Cyrl\": \"kaz\", \"uzb_Latn\": \"uzb\", \"khk_Cyrl\": \"khk\",\n}\n\nNLLB_TO_ASJP = {\n    \"eng_Latn\": \"ENGLISH\", \"spa_Latn\": \"SPANISH\", \"fra_Latn\": \"FRENCH\",\n    \"deu_Latn\": \"GERMAN_ST\", \"ita_Latn\": \"ITALIAN\", \"por_Latn\": \"PORTUGUESE\",\n    \"rus_Cyrl\": \"RUSSIAN\", \"pol_Latn\": \"POLISH\", \"hin_Deva\": \"HINDI\",\n    \"pes_Arab\": \"PERSIAN\", \"ell_Grek\": \"GREEK_MOD\", \"ron_Latn\": \"ROMANIAN\",\n    \"nld_Latn\": \"DUTCH\", \"swe_Latn\": \"SWEDISH\", \"ben_Beng\": \"BENGALI\",\n    \"zho_Hans\": \"MANDARIN\", \"jpn_Jpan\": \"JAPANESE\", \"kor_Hang\": \"KOREAN\",\n    \"arb_Arab\": \"ARABIC_ST\", \"heb_Hebr\": \"HEBREW_MOD\", \"tur_Latn\": \"TURKISH\",\n    \"vie_Latn\": \"VIETNAMESE\", \"tha_Thai\": \"THAI\", \"ind_Latn\": \"INDONESIAN\",\n    \"tgl_Latn\": \"TAGALOG\", \"swh_Latn\": \"SWAHILI\", \"yor_Latn\": \"YORUBA\",\n    \"hau_Latn\": \"HAUSA\", \"fin_Latn\": \"FINNISH\", \"hun_Latn\": \"HUNGARIAN\",\n    \"kat_Geor\": \"GEORGIAN\", \"tam_Taml\": \"TAMIL\", \"tel_Telu\": \"TELUGU\",\n    \"mya_Mymr\": \"BURMESE\", \"khm_Khmr\": \"KHMER\", \"amh_Ethi\": \"AMHARIC\",\n    \"eus_Latn\": \"BASQUE\", \"kaz_Cyrl\": \"KAZAKH\", \"uzb_Latn\": \"UZBEK\",\n    \"khk_Cyrl\": \"MONGOLIAN_HALH\",\n}\n\n# Approximate ASJP LDND phonetic distances between language families.\n# These are placeholders; for production use, load from the ASJP database download.\n# Source: Jäger (2018), ASJP v19 pairwise LDND values.\n# Format: (lang_a_nllb, lang_b_nllb) -> LDND distance (0 = identical, 1 = maximally distant)\n# Full matrix should be loaded from asjp.clld.org exports.\n\nLANGUAGE_FAMILY_MAP = {\n    \"eng_Latn\": \"Indo-European\", \"spa_Latn\": \"Indo-European\",\n    \"fra_Latn\": \"Indo-European\", \"deu_Latn\": \"Indo-European\",\n    \"ita_Latn\": \"Indo-European\", \"por_Latn\": \"Indo-European\",\n    \"rus_Cyrl\": \"Indo-European\", \"pol_Latn\": \"Indo-European\",\n    \"hin_Deva\": \"Indo-European\", \"pes_Arab\": \"Indo-European\",\n    \"ell_Grek\": \"Indo-European\", \"ron_Latn\": \"Indo-European\",\n    \"nld_Latn\": \"Indo-European\", \"swe_Latn\": \"Indo-European\",\n    \"ben_Beng\": \"Indo-European\",\n    \"zho_Hans\": \"Sino-Tibetan\", \"mya_Mymr\": \"Sino-Tibetan\",\n    \"jpn_Jpan\": \"Japonic\", \"kor_Hang\": \"Koreanic\",\n    \"arb_Arab\": \"Afro-Asiatic\", \"heb_Hebr\": \"Afro-Asiatic\",\n    \"hau_Latn\": \"Afro-Asiatic\", \"amh_Ethi\": \"Afro-Asiatic\",\n    \"tur_Latn\": \"Turkic\", \"kaz_Cyrl\": \"Turkic\", \"uzb_Latn\": \"Turkic\",\n    \"vie_Latn\": \"Austroasiatic\", \"khm_Khmr\": \"Austroasiatic\",\n    \"tha_Thai\": \"Tai-Kadai\",\n    \"ind_Latn\": \"Austronesian\", \"tgl_Latn\": \"Austronesian\",\n    \"swh_Latn\": \"Niger-Congo\", \"yor_Latn\": \"Niger-Congo\",\n    \"fin_Latn\": \"Uralic\", \"hun_Latn\": \"Uralic\",\n    \"kat_Geor\": \"Kartvelian\", \"eus_Latn\": \"Isolate\",\n    \"tam_Taml\": \"Dravidian\", \"tel_Telu\": \"Dravidian\",\n    \"khk_Cyrl\": \"Mongolic\",\n}\n\ndef mantel_test(\n    dist_a: np.ndarray, dist_b: np.ndarray, permutations: int = 9999\n) -> dict[str, float]:\n    \"\"\"Mantel test: correlation between two distance matrices.\n\n    Uses Spearman rank correlation on the upper triangle, with a\n    permutation test for significance.\n\n    Returns dict with keys: rho, p_value, permutations.\n    \"\"\"\n    n = dist_a.shape[0]\n    assert dist_a.shape == dist_b.shape == (n, n), \"Distance matrices must be square and same size\"\n\n    idx_upper = [(i, j) for i in range(n) for j in range(i + 1, n)]\n    a_flat = np.array([dist_a[i, j] for i, j in idx_upper])\n    b_flat = np.array([dist_b[i, j] for i, j in idx_upper])\n\n    rho_obs, _ = spearmanr(a_flat, b_flat)\n\n    count_ge = 0\n    rng = np.random.default_rng(42)\n    for _ in range(permutations):\n        perm = rng.permutation(n)\n        b_perm = dist_b[np.ix_(perm, perm)]\n        b_perm_flat = np.array([b_perm[i, j] for i, j in idx_upper])\n        rho_perm, _ = spearmanr(a_flat, b_perm_flat)\n        if rho_perm >= rho_obs:\n            count_ge += 1\n\n    p_value = (count_ge + 1) / (permutations + 1)\n    return {\"rho\": float(rho_obs), \"p_value\": float(p_value), \"permutations\": permutations}\n\ndef cosine_distance_matrix(vectors: list[np.ndarray]) -> np.ndarray:\n    \"\"\"Compute pairwise cosine distance (1 - cosine_similarity) matrix.\"\"\"\n    array = np.array(vectors)\n    norms = np.linalg.norm(array, axis=1, keepdims=True)\n    normalized = array / np.clip(norms, 1e-8, None)\n    similarity = normalized @ normalized.T\n    return 1.0 - similarity\n\ndef compute_swadesh_embedding_distances(\n    concept_embeddings: dict[str, dict[str, np.ndarray]],\n    languages: list[str],\n) -> np.ndarray:\n    \"\"\"Compute mean cosine distance across Swadesh concepts for each language pair.\n\n    concept_embeddings: {concept_name: {lang_code: embedding_vector}}\n    languages: ordered list of NLLB language codes\n\n    Returns: N×N distance matrix.\n    \"\"\"\n    n = len(languages)\n    lang_idx = {lang: i for i, lang in enumerate(languages)}\n    dist_sum = np.zeros((n, n))\n    count = np.zeros((n, n))\n\n    for concept, lang_vecs in concept_embeddings.items():\n        available = [lang for lang in languages if lang in lang_vecs]\n        if len(available) < 2:\n            continue\n        vecs = [lang_vecs[lang] for lang in available]\n        cos_dist = cosine_distance_matrix(vecs)\n        for ai, lang_a in enumerate(available):\n            for bi, lang_b in enumerate(available):\n                i, j = lang_idx[lang_a], lang_idx[lang_b]\n                dist_sum[i, j] += cos_dist[ai, bi]\n                count[i, j] += 1\n\n    count = np.clip(count, 1, None)\n    return dist_sum / count\n\ndef load_swadesh_corpus() -> dict[str, Any]:\n    \"\"\"Load the Swadesh 100 corpus from the data directory.\"\"\"\n    path = DATA_DIR / \"swadesh_100.json\"\n    with open(path) as f:\n        return json.load(f)\n\ndef load_color_terms() -> dict[str, Any]:\n    \"\"\"Load the Berlin & Kay color terms from the data directory.\"\"\"\n    path = DATA_DIR / \"color_terms.json\"\n    with open(path) as f:\n        return json.load(f)\n\ndef swadesh_convergence_ranking(\n    concept_embeddings: dict[str, dict[str, np.ndarray]],\n    languages: list[str],\n) -> list[dict[str, Any]]:\n    \"\"\"Rank Swadesh concepts by mean cross-lingual cosine similarity.\n\n    Returns a sorted list of dicts: [{concept, mean_similarity, std, n_languages}, ...].\n    Higher mean_similarity = stronger cross-lingual convergence.\n    \"\"\"\n    results = []\n    for concept, lang_vecs in concept_embeddings.items():\n        available = [lang for lang in languages if lang in lang_vecs]\n        if len(available) < 2:\n            continue\n        vecs = [lang_vecs[lang] for lang in available]\n        array = np.array(vecs)\n        norms = np.linalg.norm(array, axis=1, keepdims=True)\n        normalized = array / np.clip(norms, 1e-8, None)\n        sim = normalized @ normalized.T\n        upper = [sim[i, j] for i in range(len(available)) for j in range(i + 1, len(available))]\n        results.append({\n            \"concept\": concept,\n            \"mean_similarity\": float(np.mean(upper)),\n            \"std\": float(np.std(upper)),\n            \"n_languages\": len(available),\n        })\n    results.sort(key=lambda x: x[\"mean_similarity\"], reverse=True)\n    return results\n\n# ---------------------------------------------------------------------------\n# CLICS² Colexification helpers\n# ---------------------------------------------------------------------------\n\n# Top 50 frequently colexified concept pairs from CLICS² (List et al., 2018)\n# These pairs are lexified by the same word in many unrelated languages.\nCOLEXIFIED_PAIRS = [\n    (\"hand\", \"arm\"), (\"foot\", \"leg\"), (\"skin\", \"bark\"), (\"tree\", \"wood\"),\n    (\"hear\", \"feel\"), (\"eye\", \"see\"), (\"mouth\", \"lip\"), (\"sun\", \"day\"),\n    (\"moon\", \"month\"), (\"star\", \"sky\"), (\"water\", \"river\"), (\"fire\", \"burn\"),\n    (\"earth\", \"ground\"), (\"stone\", \"rock\"), (\"seed\", \"fruit\"), (\"leaf\", \"feather\"),\n    (\"root\", \"origin\"), (\"bone\", \"body\"), (\"blood\", \"red\"), (\"tongue\", \"language\"),\n    (\"nose\", \"smell\"), (\"tooth\", \"bite\"), (\"knee\", \"bend\"), (\"horn\", \"mountain\"),\n    (\"tail\", \"end\"), (\"egg\", \"round\"), (\"liver\", \"anger\"), (\"heart\", \"mind\"),\n    (\"neck\", \"throat\"), (\"belly\", \"stomach\"), (\"breast\", \"chest\"), (\"flesh\", \"meat\"),\n    (\"hair\", \"fur\"), (\"claw\", \"nail\"), (\"rain\", \"cloud\"), (\"smoke\", \"fog\"),\n    (\"path\", \"road\"), (\"big\", \"many\"), (\"long\", \"tall\"), (\"small\", \"narrow\"),\n    (\"new\", \"young\"), (\"hot\", \"warm\"), (\"cold\", \"ice\"), (\"good\", \"beautiful\"),\n    (\"night\", \"dark\"), (\"die\", \"dead\"), (\"eat\", \"food\"), (\"drink\", \"water\"),\n    (\"sleep\", \"dream\"), (\"come\", \"arrive\"),\n]\n\n# 50 concept pairs that are NEVER colexified across languages.\nNON_COLEXIFIED_PAIRS = [\n    (\"hand\", \"mountain\"), (\"foot\", \"star\"), (\"skin\", \"fire\"), (\"tree\", \"tooth\"),\n    (\"eye\", \"stone\"), (\"mouth\", \"moon\"), (\"sun\", \"bone\"), (\"water\", \"feather\"),\n    (\"earth\", \"tongue\"), (\"cloud\", \"blood\"), (\"smoke\", \"seed\"), (\"fire\", \"fish\"),\n    (\"path\", \"egg\"), (\"mountain\", \"belly\"), (\"red\", \"sleep\"), (\"green\", \"die\"),\n    (\"yellow\", \"swim\"), (\"white\", \"eat\"), (\"black\", \"fly\"), (\"night\", \"horn\"),\n    (\"hot\", \"tail\"), (\"cold\", \"claw\"), (\"full\", \"sand\"), (\"new\", \"ash\"),\n    (\"good\", \"louse\"), (\"round\", \"woman\"), (\"dry\", \"dog\"), (\"name\", \"grease\"),\n    (\"big\", \"knee\"), (\"long\", \"neck\"), (\"small\", \"liver\"), (\"many\", \"breast\"),\n    (\"one\", \"rain\"), (\"two\", \"smoke\"), (\"man\", \"leaf\"), (\"woman\", \"root\"),\n    (\"person\", \"bark\"), (\"fish\", \"hair\"), (\"bird\", \"belly\"), (\"dog\", \"ear\"),\n    (\"tree\", \"eye\"), (\"seed\", \"nose\"), (\"leaf\", \"mouth\"), (\"root\", \"tooth\"),\n    (\"skin\", \"tongue\"), (\"flesh\", \"foot\"), (\"blood\", \"knee\"), (\"bone\", \"hand\"),\n    (\"grease\", \"head\"), (\"egg\", \"neck\"),\n]\n\ndef colexification_test(\n    concept_embeddings: dict[str, dict[str, np.ndarray]],\n    languages: list[str],\n) -> dict[str, Any]:\n    \"\"\"Mann-Whitney U test comparing cosine similarity of colexified vs non-colexified pairs.\n\n    For each concept pair, computes mean cross-lingual cosine similarity of the\n    two concepts' embeddings averaged across languages, then compares the\n    distributions of colexified vs non-colexified pairs.\n\n    Returns dict with: colexified_mean, non_colexified_mean, U_statistic, p_value,\n                       colexified_sims, non_colexified_sims.\n    \"\"\"\n\n    def _pair_similarity(concept_a: str, concept_b: str) -> float | None:\n        sims = []\n        for lang in languages:\n            vec_a = concept_embeddings.get(concept_a, {}).get(lang)\n            vec_b = concept_embeddings.get(concept_b, {}).get(lang)\n            if vec_a is None or vec_b is None:\n                continue\n            norm_a = np.linalg.norm(vec_a)\n            norm_b = np.linalg.norm(vec_b)\n            if norm_a < 1e-8 or norm_b < 1e-8:\n                continue\n            sims.append(float(np.dot(vec_a, vec_b) / (norm_a * norm_b)))\n        return float(np.mean(sims)) if sims else None\n\n    colex_sims = []\n    for a, b in COLEXIFIED_PAIRS:\n        sim = _pair_similarity(a, b)\n        if sim is not None:\n            colex_sims.append(sim)\n\n    non_colex_sims = []\n    for a, b in NON_COLEXIFIED_PAIRS:\n        sim = _pair_similarity(a, b)\n        if sim is not None:\n            non_colex_sims.append(sim)\n\n    if len(colex_sims) < 2 or len(non_colex_sims) < 2:\n        return {\n            \"error\": \"Insufficient concept overlap with embeddings\",\n            \"colexified_count\": len(colex_sims),\n            \"non_colexified_count\": len(non_colex_sims),\n        }\n\n    u_stat, p_value = mannwhitneyu(colex_sims, non_colex_sims, alternative=\"greater\")\n\n    return {\n        \"colexified_mean\": float(np.mean(colex_sims)),\n        \"colexified_std\": float(np.std(colex_sims)),\n        \"non_colexified_mean\": float(np.mean(non_colex_sims)),\n        \"non_colexified_std\": float(np.std(non_colex_sims)),\n        \"U_statistic\": float(u_stat),\n        \"p_value\": float(p_value),\n        \"colexified_sims\": colex_sims,\n        \"non_colexified_sims\": non_colex_sims,\n        \"colexified_count\": len(colex_sims),\n        \"non_colexified_count\": len(non_colex_sims),\n    }\n```\n\nIMPORTANT:\n- Create the file at `/Users/kylemathewson/InterpretCognates/backend/app/benchmarks.py`\n- The file depends on scipy (scipy.stats.spearmanr, scipy.stats.mannwhitneyu) — these will be added to requirements.txt separately\n- Make sure the code is syntactically correct Python\n- Return a summary of all functions and their signatures when done\n</user_query>"}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"The user wants me to create a new file at `/Users/kylemathewson/InterpretCognates/backend/app/benchmarks.py` with the provided content. Let me first check that the parent directory exists."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"The directory exists. I'll create the file now."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Let me verify the file was created correctly by checking its syntax."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"The file has been created and passes syntax validation. Here is a summary of all functions and their signatures:\n\n**Functions:**\n\n| Function | Signature | Description |\n|---|---|---|\n| `mantel_test` | `(dist_a: np.ndarray, dist_b: np.ndarray, permutations: int = 9999) -> dict[str, float]` | Mantel test — Spearman rank correlation between two distance matrices with permutation-based p-value. |\n| `cosine_distance_matrix` | `(vectors: list[np.ndarray]) -> np.ndarray` | Computes pairwise cosine distance (1 − cosine similarity) matrix from a list of vectors. |\n| `compute_swadesh_embedding_distances` | `(concept_embeddings: dict[str, dict[str, np.ndarray]], languages: list[str]) -> np.ndarray` | Computes mean cosine distance across Swadesh concepts for each language pair, returning an N×N distance matrix. |\n| `load_swadesh_corpus` | `() -> dict[str, Any]` | Loads the Swadesh 100 corpus from `DATA_DIR / \"swadesh_100.json\"`. |\n| `load_color_terms` | `() -> dict[str, Any]` | Loads Berlin & Kay color terms from `DATA_DIR / \"color_terms.json\"`. |\n| `swadesh_convergence_ranking` | `(concept_embeddings: dict[str, dict[str, np.ndarray]], languages: list[str]) -> list[dict[str, Any]]` | Ranks Swadesh concepts by mean cross-lingual cosine similarity (highest convergence first). |\n| `colexification_test` | `(concept_embeddings: dict[str, dict[str, np.ndarray]], languages: list[str]) -> dict[str, Any]` | Mann-Whitney U test comparing cosine similarity distributions of CLICS² colexified vs non-colexified concept pairs. |\n\n**Constants/Data:**\n\n- `NLLB_TO_ISO` — 40-language mapping from NLLB BCP-47 codes to ISO 639-3\n- `NLLB_TO_ASJP` — 40-language mapping from NLLB codes to ASJP language names\n- `LANGUAGE_FAMILY_MAP` — 40-language mapping from NLLB codes to language family names\n- `COLEXIFIED_PAIRS` — 50 frequently colexified concept pairs from CLICS²\n- `NON_COLEXIFIED_PAIRS` — 50 never-colexified concept pairs (negative controls)"}]}}