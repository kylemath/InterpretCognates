{"role":"user","message":{"content":[{"type":"text","text":"<user_query>\nYou are working on the InterpretCognates project at /Users/kylemathewson/InterpretCognates.\n\nYOUR TASK: Create ONE file â€” `research/sprint1_report.md` â€” documenting all the work completed in this sprint. This is a follow-up to the existing `research/executive_summary.md`.\n\nRead these files first for context:\n- `research/executive_summary.md` (the original research meeting summary and roadmap)\n- `backend/app/static/swadesh_detail.html` (the Swadesh detail page structure)\n- `backend/app/static/swadesh_detail.js` (orthographic/phonetic analysis, categories, ranking)\n- `backend/app/static/validation.html` (the validation tests page)\n- `backend/app/static/validation.js` (the 5-6 validation experiments)\n- `backend/app/benchmarks.py` (all benchmark functions)\n- `backend/app/main.py` (all API endpoints)\n- `backend/app/data/non_swadesh_60.json` (just read the metadata and first few concepts)\n\n## Document Structure:\n\n# InterpretCognates â€” Sprint 1 Report\n**Date: February 23, 2026**\n**Follow-up to: Executive Summary of February 22, 2026**\n\n---\n\n## 1. Sprint Overview\nBrief summary of what was accomplished in this sprint. Reference the executive summary's recommended first sprint (Section 7) and note what was completed, what was deferred, and what novel directions emerged.\n\n## 2. Infrastructure Built\n\n### 2.1 Swadesh Convergence Detail Page (/swadesh)\n- Full 101-concept ranking visualization\n- Three sortable metrics: embedding convergence, orthographic similarity, phonetic similarity\n- Semantic category grouping (8 categories: Pronouns, Body, Nature, Living Things, Colors, Actions, Properties, People)\n- Column-header sorting with grouped/flat toggle\n- Orthographic similarity: normalized Levenshtein distance across 21 Latin-script language pairs (210 pairs per concept)\n- Phonetic similarity (approximate): diacritic stripping, voiced/voiceless merging, geminate collapsing, then Levenshtein on normalized forms\n- Variance decomposition scatter plots: orthographic vs embedding and phonetic vs embedding, with regression lines and semantic surplus annotations\n- Category summary grouped bar chart (all three metrics)\n- Isotropy correction toggle with localStorage caching\n\n### 2.2 Validation & Literature Tests Page (/validation)\n- Panel 1: Isotropy correction stability test\n- Panel 2: Swadesh vs non-Swadesh comparison (Mann-Whitney U)\n- Panel 3: CLICSÂ² colexification test\n- Panel 4: Conceptual store metric (mean-centering between/within ratio)\n- Panel 5: Phylogenetic distance matrix preparation\n- Panel 6: Semantic offset invariance test\n\n### 2.3 Backend Additions\nList all new API endpoints and benchmark functions added.\n\n### 2.4 Data Assets Created\n- non_swadesh_60.json: 60 culturally-specific concepts Ã— 40 languages (2,400 translations)\n- Semantic category assignments for all 101 Swadesh concepts\n- DEFAULT_OFFSET_PAIRS: 15 concept pairs for offset invariance testing\n\n## 3. Scientific Progress Against the Roadmap\n\nMap each of the executive summary's 10 tasks against what was accomplished:\n- Task 1 (ABTT Isotropy): âœ… Implemented, integrated as toggle + validation test\n- Task 2 (Mean-Centering PCA): âœ… Already existed, now has quantitative metric endpoint\n- Task 3 (Per-Layer Extraction): â¬œ Deferred (infrastructure exists from prior sprint)\n- Task 4 (Swadesh Corpus & Batch): âœ… Complete â€” convergence ranking operational, full detail page\n- Task 5 (Per-Head Attention): â¬œ Deferred\n- Task 6 (Phylogenetic Mantel): ðŸŸ¡ Partial â€” embedding distance matrix ready, ASJP data not yet integrated\n- Task 7 (Cognate Entropy): â¬œ Deferred\n- Task 8 (Color Circle): âœ… Already operational from prior sprint\n- Task 9 (CLICSÂ² Colexification): âœ… Integrated into validation page\n- Task 10 (Neuron Overlap): â¬œ Deferred\n\n## 4. Novel Directions Not in Original Roadmap\n\n### 4.1 Orthographic/Phonetic Variance Decomposition\nDescribe the analytical framework: regressing embedding convergence on surface-form similarity to identify \"semantic surplus\" â€” concepts that converge beyond what orthography/phonetics would predict. This addresses the key confound that high convergence could reflect cognate overlap rather than conceptual universality.\n\n### 4.2 Swadesh vs Non-Swadesh Comparison\nDescribe the direct test of Swadesh's cultural stability hypothesis: comparing convergence distributions of core vs non-core vocabulary. This was identified during the sprint as the single most publishable test available.\n\n### 4.3 Semantic Offset Invariance\nDescribe this novel experiment: testing whether semantic relationships (manâ†’woman, bigâ†’small, eatâ†’drink) are translationally invariant. For each concept pair, offset vectors are computed per language and their parallelism measured. This goes beyond first-order convergence (do translations cluster together?) to second-order structure (are relationships between concepts preserved across languages?). No prior paper has tested this on 40+ languages in a translation model.\n\nKey insight from the sprint discussion: the consistency of offset vectors may correlate with Swadesh-style cultural fundamentality â€” pragmatically universal relationships (Iâ†’we, manâ†’woman) should be more invariant than culturally contingent ones.\n\n## 5. Key Analytical Insights\n\n### 5.1 The Polysemy Confound\nNote that bottom-ranked Swadesh items (bark, lie, see, give) are disproportionately polysemous. Low convergence may reflect translation ambiguity, not cultural instability. Future work should control for sense count (e.g., from WordNet).\n\n### 5.2 The Orthographic Decomposition Finding\nThe variance decomposition scatter plot reveals which concepts have high \"semantic surplus\" (convergence beyond what surface form predicts) vs \"semantic deficit\" (less convergence than orthography would suggest). Concepts with high surplus are the strongest candidates for genuine conceptual universals.\n\n### 5.3 Approximate Phonetic Similarity\nNote the limitations: the phonetic approximation (diacritic stripping + voicing neutralization) is not true IPA. Integration of ASJP per-word phonetic transcriptions would provide a proper phonetic dimension. The current approximation captures broad equivalences but misses language-specific phonological processes.\n\n### 5.4 Distributional Geometry\nDiscussion of what the shape of per-concept embedding clouds reveals: spherical distributions suggest equal deviation across languages, elongated distributions suggest language-family gradients. After mean-centering, clouds should become more isotropic. The conceptual store metric quantifies this.\n\n## 6. Hypotheses Status\n\nUpdate the three novel hypotheses from the executive summary:\n\n**Hypothesis 1 (RHM-Resource Asymmetry):** Not yet tested. Requires resource-level metadata for NLLB languages.\n\n**Hypothesis 2 (Conceptual Store Mean-Centering):** Quantitative metric endpoint built. The 2Ã— improvement factor prediction is now directly testable via the validation page.\n\n**Hypothesis 3 (Phylogenetic Correlation):** Embedding distance matrix ready. ASJP distance matrix needed for the Mantel test. Infrastructure is 80% complete.\n\n**New Hypothesis 4 (Offset Invariance):** Semantic offset vectors for culturally fundamental concept pairs will show significantly higher cross-lingual consistency (cosine similarity > 0.7) than control pairs. This consistency will correlate with the Swadesh convergence ranking of the constituent concepts.\n\n## 7. Recommended Next Steps (Sprint 2)\n\n1. **Run all validation tests** and document quantitative results\n2. **Integrate ASJP data** for the phylogenetic Mantel test (highest-impact publication finding)\n3. **Add polysemy controls** using WordNet sense counts as a covariate\n4. **Integrate IPA/ASJP per-word data** for true phonetic similarity\n5. **Test Hypothesis 4** â€” correlate offset invariance with Swadesh convergence ranking\n6. **Per-layer analysis** (Task 3) â€” trace how convergence and offset invariance change across encoder layers\n7. **Write up** the Swadesh convergence + variance decomposition + Swadesh vs non-Swadesh comparison as a short paper\n\n## 8. Technical Debt\n- Python 3.9 type hint compatibility (avoid X | Y syntax)\n- No automated tests for benchmark functions\n- Embedding computation is slow (~2-5 min per experiment); caching strategy needed\n- Non-Swadesh translations are AI-generated and should be spot-checked\n\n---\n\n*Sprint 1 report compiled February 23, 2026.*\n*Reference: executive_summary.md (February 22, 2026)*\n\n---\n\nIMPORTANT NOTES:\n- Write this as a real research document, not a template. Fill in specific details from reading the actual code.\n- Reference actual function names, file paths, and endpoint URLs.\n- Be specific about what exists in the code vs what still needs data/testing.\n- Maintain the scholarly tone of the executive summary.\n- The report should be useful to someone who hasn't seen the codebase â€” include enough technical detail to reproduce.\n- This is a MARKDOWN file. Use proper headings, lists, and formatting.\n\nReturn confirmation when the file is written.\n</user_query>"}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Let me read all the required files first to gather context for writing the sprint report."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Let me also check the modeling module and confirm the data directory contents.\n\nNow I have a thorough understanding of all the files. Let me also check what other files exist in the data directory and look at the modeling.py for completeness."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Now I have all the context I need. Let me read the rest of modeling.py to get the full picture of what's implemented."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Now I have complete context on all the files. Let me write the sprint report."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"The file `research/sprint1_report.md` has been written. It covers all eight sections as specified, with details drawn directly from the actual codebase:\n\n- **Section 1** summarizes what was accomplished vs. the original sprint plan\n- **Section 2** documents all infrastructure: the Swadesh detail page (with its three metrics, eight categories, variance decomposition, and isotropy toggle), the six-panel validation page (each panel's endpoint, backend function, and frontend rendering), all 16 `modeling.py` functions, 9 `benchmarks.py` functions with data constants, 16 API endpoints, and 3 JSON data assets\n- **Section 3** maps all 10 roadmap tasks to completion status (6 complete, 3 backend-complete, 1 partial)\n- **Section 4** describes three novel directions not in the original roadmap: orthographic/phonetic variance decomposition, Swadesh vs. non-Swadesh comparison, and semantic offset invariance\n- **Section 5** covers analytical insights: the polysemy confound, orthographic decomposition framework, phonetic approximation limitations, and distributional geometry\n- **Section 6** updates all four hypotheses with their current testability status\n- **Section 7** prioritizes seven Sprint 2 recommendations\n- **Section 8** identifies seven items of technical debt, including the duplicate `showLoading` definition in `validation.js`"}]}}