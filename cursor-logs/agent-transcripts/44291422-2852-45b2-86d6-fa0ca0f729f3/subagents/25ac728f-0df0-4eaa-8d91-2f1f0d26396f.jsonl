{"role":"user","message":{"content":[{"type":"text","text":"<user_query>\nYou are working on the InterpretCognates project at /Users/kylemathewson/InterpretCognates.\n\nYOUR TASK: Add a new \"Semantic Offset Invariance\" experiment to the backend. Modify ONLY `backend/app/main.py` and `backend/app/benchmarks.py`.\n\nIMPORTANT: Python 3.9 environment. Use `Optional`, `Dict`, `List`, `Tuple` from typing. Do NOT use `X | Y` union syntax.\n\nRead both files first to understand the existing patterns, imports, and coding style.\n\n## Concept:\nFor concept pairs like (man, woman), we can compute the offset vector (embed(\"woman\") - embed(\"man\")) in each language independently. If the model has a language-neutral conceptual space, these offset vectors should be parallel across languages. We measure this by computing cosine similarity between each language's offset vector and the centroid offset vector.\n\n## In benchmarks.py, add:\n\n### semantic_offset_invariance(concept_embeddings, languages, concept_pairs)\nParameters:\n- concept_embeddings: Dict[str, Dict[str, np.ndarray]] — {concept_name: {lang_code: embedding}}\n- languages: List[str] — language codes\n- concept_pairs: List[Tuple[str, str]] — pairs like [(\"man\", \"woman\"), (\"one\", \"two\")]\n\nFor each pair (concept_a, concept_b):\n1. For each language that has both concepts, compute offset = embed(concept_b) - embed(concept_a)\n2. Compute the centroid offset (mean of all language offsets)\n3. For each language, compute cosine similarity between its offset and the centroid offset\n4. Group languages by family using the existing LANGUAGE_FAMILY_MAP dict\n5. Compute mean consistency (mean cosine similarity across all languages)\n6. Compute per-family mean consistency\n\nReturn a list of dicts, one per pair:\n```python\n{\n    \"concept_a\": str,\n    \"concept_b\": str,\n    \"centroid_offset_norm\": float,  # magnitude of centroid offset\n    \"mean_consistency\": float,  # mean cosine sim of per-lang offsets vs centroid\n    \"std_consistency\": float,\n    \"n_languages\": int,\n    \"per_language\": [{\"lang\": str, \"family\": str, \"consistency\": float}],\n    \"per_family\": [{\"family\": str, \"mean_consistency\": float, \"n\": int}],\n}\n```\n\n### Define DEFAULT_OFFSET_PAIRS at module level:\n```python\nDEFAULT_OFFSET_PAIRS = [\n    (\"man\", \"woman\"),      # gender\n    (\"one\", \"two\"),        # numerosity\n    (\"I\", \"we\"),           # singular→plural person\n    (\"sun\", \"moon\"),       # celestial pair\n    (\"fire\", \"water\"),     # elemental opposition\n    (\"big\", \"small\"),      # size antonymy\n    (\"hot\", \"cold\"),       # temperature antonymy\n    (\"good\", \"new\"),       # abstract property (control - no expected invariance)\n    (\"dog\", \"fish\"),       # animal pair (control)\n    (\"die\", \"kill\"),       # causative pair\n    (\"come\", \"give\"),      # motion/transfer (control)\n    (\"eye\", \"ear\"),        # sensory organ pair\n    (\"black\", \"white\"),    # color opposition\n    (\"night\", \"sun\"),      # temporal opposition\n    (\"eat\", \"drink\"),      # consumptive pair\n]\n```\n\n## In main.py, add:\n\n### POST /api/experiment/offset-invariance\n- Load the Swadesh corpus\n- Embed all concepts using embed_text_batch (same pattern as other Swadesh experiments)\n- Call semantic_offset_invariance() with DEFAULT_OFFSET_PAIRS\n- Response: { num_pairs, num_languages, pairs: [the list from above] }\n\n### Add imports:\n- Import semantic_offset_invariance and DEFAULT_OFFSET_PAIRS from benchmarks\n\nKeep the same coding style as existing endpoints. Use the existing _embed_corpus helper if one exists, or follow the same embedding pattern as experiment_swadesh.\n\nReturn a summary of what you added.\n</user_query>"}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Let me start by reading both files to understand the existing patterns, imports, and coding style."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Now I have a clear picture of both files. Let me implement the changes.\n\nNow I have a clear picture of both files. Let me implement the changes:\n\n1. **benchmarks.py**: Add `DEFAULT_OFFSET_PAIRS` constant and `semantic_offset_invariance()` function\n2. **main.py**: Add the `/api/experiment/offset-invariance` endpoint with proper imports\n\nI need to be careful about Python 3.9 compatibility - use `Dict`, `List`, `Tuple` from typing, not `dict[str, ...]` syntax. I notice the existing code actually uses both styles inconsistently - some functions use `dict[str, ...]` (lowercase) and some use `Dict[str, ...]` (uppercase from typing). I'll use the uppercase typing versions as instructed.\n\n\nI also need to update the typing imports in benchmarks.py to include `Tuple` since I'll be using it in the new function signature."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Now add `DEFAULT_OFFSET_PAIRS` and `semantic_offset_invariance` at the end of `benchmarks.py`:"}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Now update `main.py` — add the imports and the new endpoint:"}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Now add the new endpoint. I'll place it after the conceptual-store-metric endpoint and before the validation page route:"}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Let me verify both files look correct:"}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"No linter errors. Let me do a quick sanity check on the final state of both files:"}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Everything looks clean. Here's a summary of what was added:\n\n**`backend/app/benchmarks.py`:**\n- Added `Tuple` to the `typing` imports\n- Added `DEFAULT_OFFSET_PAIRS` — a module-level list of 15 concept pairs spanning gender, numerosity, antonymy, causation, and control pairs\n- Added `semantic_offset_invariance()` function that:\n  1. For each pair, computes the offset vector `embed(b) - embed(a)` per language\n  2. Computes the centroid offset (mean across languages)\n  3. Measures cosine similarity of each language's offset against the centroid\n  4. Groups languages by family using `LANGUAGE_FAMILY_MAP` and computes per-family stats\n  5. Returns a list of result dicts with `mean_consistency`, `std_consistency`, `per_language`, and `per_family` breakdowns\n\n**`backend/app/main.py`:**\n- Added `DEFAULT_OFFSET_PAIRS` and `semantic_offset_invariance` to the benchmarks import block\n- Added `POST /api/experiment/offset-invariance` endpoint that loads the Swadesh corpus, embeds it via the existing `_embed_corpus` helper, calls `semantic_offset_invariance`, and returns `{num_pairs, num_languages, pairs}`"}]}}