{"role":"user","message":{"content":[{"type":"text","text":"<user_query>\nYou are implementing real ASJP phonetic distance computation to replace a fabricated taxonomy-based approximation.\n\n## Context\nThe project is at /Users/kylemathewson/InterpretCognates/backend/\nThe ASJP v20 data has been downloaded and extracted to:\n  /Users/kylemathewson/InterpretCognates/backend/app/data/external/asjp/lexibank-asjp-f0f1d0d/\n\nThe data is in CLDF format:\n- `cldf/forms.csv`: ~482K rows. Columns: ID, Local_ID, Language_ID, Parameter_ID, Value, Form, Segments, Comment, Source, Cognacy, Loan, Graphemes, Profile, gloss_in_source\n- `cldf/languages.csv`: ~10K rows. Columns: ID, Name, Glottocode, Glottolog_Name, ISO639P3code, Macroarea, Latitude, Longitude, Family, ...\n- `cldf/parameters.csv`: Concept list. Columns: ID, Name, Concepticon_ID, Concepticon_Gloss (e.g., \"1,*I,1209,I\" \"2,*you,1215,THOU\")\n\n## Current fabricated code to replace\nIn `/Users/kylemathewson/InterpretCognates/backend/app/benchmarks.py`, the function `compute_asjp_distance_matrix()` (around line 821) uses hardcoded taxonomy-based distances with random noise. It needs to be replaced with real ASJP LDND computation.\n\nThe function signature is:\n```python\ndef compute_asjp_distance_matrix(languages: List[str]) -> Tuple[np.ndarray, List[str]]:\n```\nWhere `languages` is a list of NLLB language codes like \"eng_Latn\", \"fra_Latn\", etc.\n\nThere's already a mapping `NLLB_TO_ASJP` in benchmarks.py that maps NLLB codes to ASJP language identifiers (ISO 639-3 codes).\n\n## What you need to do\n\n1. First, read the current `NLLB_TO_ASJP` mapping in benchmarks.py to understand what ASJP identifiers are used.\n\n2. Create a new module or function that:\n   a. Loads ASJP word lists from the CLDF forms.csv, using ISO639P3code from languages.csv to map to our NLLB codes\n   b. For each pair of languages, computes the REAL Normalized Levenshtein Distance (NLD) across all shared Swadesh items\n   c. The ASJP distance between two languages = mean NLD across all shared word pairs\n   d. NLD for two words = levenshtein_distance(word_a, word_b) / max(len(word_a), len(word_b))\n\n3. Replace the fabricated `compute_asjp_distance_matrix()` function with one that loads and uses real ASJP data.\n\n4. Remove the hardcoded `_WITHIN_SUBFAMILY_LDND`, `_IE_CLOSE_BRANCHES`, `_IE_INTER_BRANCH_DEFAULT`, `_CROSS_FAMILY_DEFAULT` constants, and the `_broad_family()` helper that are only used by the old fabricated function.\n\n5. Also remove the random noise generation (rng = np.random.default_rng(42), etc.)\n\n## Important constraints\n- Use the virtual environment at /Users/kylemathewson/InterpretCognates/backend/venv/\n- The ASJP CLDF data uses Language_ID like \"ENGLISH\" or \"A51_BAFIA_MAJA\", and languages.csv has ISO639P3code. You need to map ISO codes to NLLB codes using the existing NLLB_TO_ASJP mapping (which maps NLLB codes to ISO codes).\n- Some languages may have multiple ASJP word lists (dialects). Pick the one with the most complete word list, or average across them.\n- The Form column in forms.csv contains the ASJP transcription of the word. Use that for distance computation.\n- Cache the loaded data so it's not re-parsed on every call.\n- Make sure the function still returns (distance_matrix, filtered_languages) with the same interface.\n- Update the docstring to cite the real ASJP source.\n\n## Testing\nAfter implementing, verify it works by running a quick test:\n```python\ncd /Users/kylemathewson/InterpretCognates/backend\nsource venv/bin/activate\npython -c \"\nfrom app.benchmarks import compute_asjp_distance_matrix\ndist, langs = compute_asjp_distance_matrix(['eng_Latn', 'fra_Latn', 'deu_Latn', 'spa_Latn', 'rus_Cyrl', 'zho_Hans', 'jpn_Jpan', 'arb_Arab', 'hin_Deva', 'swh_Latn'])\nprint(f'Languages: {langs}')\nprint(f'Matrix shape: {dist.shape}')\nprint(f'Min: {dist.min():.4f}, Max: {dist.max():.4f}, Mean: {dist.mean():.4f}')\nimport numpy as np\nfor i in range(len(langs)):\n    for j in range(i+1, len(langs)):\n        print(f'  {langs[i]:10s} <-> {langs[j]:10s}: {dist[i,j]:.4f}')\n\"\n```\n\nThe distances should be continuous (not clustered in 3 tiers), range roughly 0.3-1.0, and related languages should have lower distances.\n\nReturn the complete changes you made to benchmarks.py.\n</user_query>"}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Let me start by understanding the current code and data structure. I need to read several files to understand the current state."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Let me read the current code and data files in parallel."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Let me verify that the ASJP Language_IDs from `NLLB_TO_ASJP` actually exist in the data, and check parameters.csv fully.\n\nNow I need to understand the ASJP data structure. Let me check a few things:\n\n1. The `NLLB_TO_ASJP` mapping maps NLLB codes to ASJP Language_ID names like \"ENGLISH\", \"FRENCH\", etc.\n2. The languages.csv has Language_ID names and ISO639P3code columns.\n3. The forms.csv has Language_ID, Parameter_ID, and Form columns.\n\nI need to:\n1. Load languages.csv to build a mapping from Language_ID → ISO639P3code\n2. Build a reverse mapping from ISO639P3code → Language_ID(s) \n\n\n3. Since `NLLB_TO_ASJP` already maps NLLB codes directly to ASJP Language_ID names, I can use that mapping directly without needing additional transformations.\n\nNow I need to figure out the actual workflow: load forms.csv and filter it by the Language_ID values that correspond to my NLLB codes, then group the forms by Language_ID and Parameter_ID to compute mean NLD across shared concepts for each language pair. But I should verify whether the ASJP data actually uses Language_ID values like \"ENGLISH\" or if they're formatted differently like \"A51_BAFIA_MAJA\"."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Good, the Language_ID values match. Let me verify a few of the non-obvious ones.\n\nThe ASJP Language_ID values match exactly. Now let me check a few more:\n- GERMAN_ST, ARABIC_ST, GREEK_MOD, HEBREW_MOD, etc."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Some IDs don't match. Let me find the correct ASJP names for those languages.\n\nSome of the ASJP Language_IDs don't match. Let me look for the correct ones for GERMAN_ST, ARABIC_ST, GREEK_MOD, HEBREW_MOD, and MONGOLIAN_HALH."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Now I can see the actual ASJP Language_ID values. Many of the ones in the existing `NLLB_TO_ASJP` mapping are wrong. The actual names are:\n- GERMAN_ST → need to find the right one for Standard German (deu)\n- ARABIC_ST → ARABIC_QURANIC (arb) for Standard Arabic\n- GREEK_MOD → GREEK (ell)\n- HEBREW_MOD → HEBREW (heb)\n- MONGOLIAN_HALH → MONGOLIAN (khk)\n\nLet me search for German and a few others."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Now I have a much better picture. The `NLLB_TO_ASJP` mapping has some incorrect Language_IDs. Instead of relying on that mapping, the better approach is:\n\n1. Build a mapping from ISO639P3code → list of Language_IDs using languages.csv\n2. For each NLLB code, look up the ISO code using NLLB_TO_ISO, then find the matching ASJP Language_IDs\n3. For each ASJP Language_ID, load its word forms from forms.csv\n4. Pick the doculect with the most complete list, or average\n   \nBut I should verify whether the NLLB_TO_ASJP names can be used directly—many of them are incorrect mappings that don't match the actual ASJP doculect names, so I'll need to validate and correct these mismatches before proceeding.\n\nRather than trying to fix each name individually, I think the better approach is to use the ISO code as an intermediary: I'll map each NLLB language to its ISO 639-3 code, then use that to find all matching ASJP Language_IDs in the languages.csv file, and finally select the most complete doculect for each language.\n\nLet me search for Scots Gaelic to see what's available."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Let me check if `NLLB_TO_ASJP` is used elsewhere in the codebase and also see what other files reference benchmarks.\n\nNow I have a clear picture. The best approach is to use the ISO code mapping rather than the ASJP name mapping, since many names in `NLLB_TO_ASJP` don't match the actual ASJP data. The approach:\n\n1. Load languages.csv → build ISO639P3code → [Language_ID list] mapping\n2. Use NLLB_TO_ISO → ISO code, then look up all ASJP Language_IDs for that ISO\n3. Load forms.csv → build Language_ID → {Parameter_ID → [Form list]} mapping (only for needed languages)\n\n\n4. For each NLLB language, select the doculect with the most complete forms\n5. For each language pair, calculate mean normalized Levenshtein distance across shared parameters\n6. The ISO639P3code column is the 5th column in languages.csv, though I should verify it's distinct from any other ISO code columns in the header.\n\nFor the implementation strategy, since forms.csv has nearly half a million rows, I'll need to be strategic: load languages.csv first to map ISO codes to Language_IDs, filter down to just the codes I need, then load and filter forms.csv accordingly to avoid processing unnecessary data. I'm also tracking how NLLB codes map to ISO codes—for instance, \"arb_Arab\" corresponds to ISO arb, which encompasses multiple Language_IDs like ARABIC_QURANIC, CLASSICAL_ARABIC, and STANDARD_ARABIC variants.\n\nNow I'm noticing some inconsistencies in the data that need careful handling. When I search for \"arb\" in the languages file, I'm getting matches in different columns—ARABELA has code_wals=\"arb\" but ISO639P3code=\"arl\", while ARABIC_QURANIC has the actual ISO code \"arb\". I need to be precise about which column I'm matching against to avoid pulling the wrong language records.\n\nFor the implementation, I'm setting up a proper CSV parser to load the ASJP data and build mappings from ISO codes to language identifiers...\n\nBuilding the cache structure... I'm loading the word forms from the CSV for each language ID we need, filtering to only keep non-empty forms and organizing them by language and parameter. Then I'm iterating through each ISO code to select the doculect with the most items...\n\nImplementing the Levenshtein distance function... I'm implementing a function that loads ASJP phonetic data from the CLDF database and computes normalized Levenshtein distances between language pairs based on their Swadesh word lists.\n\nNow I'm filtering the input languages to only those with available ASJP data, then iterating through all pairs to calculate their phonetic distances by comparing word lists.\n\nComputing normalized Levenshtein distances... Now I need to switch to using `NLLB_TO_ISO` for the distance computation instead of `NLLB_TO_ASJP`, since the old mapping had incorrect entries and I'm already converting to ISO codes. Let me verify where `NLLB_TO_ASJP` is actually being used in the codebase to make sure I'm not breaking anything else."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Now I have a clear picture of the data. The key insight is that many names in `NLLB_TO_ASJP` are wrong (e.g., \"GERMAN_ST\" should be \"STANDARD_GERMAN\"), so the best approach is to use `NLLB_TO_ISO` to map via ISO 639-3 codes, which languages.csv has in its `ISO639P3code` column.\n\nLet me now implement the replacement. I'll:\n1. Remove the fabricated constants and `_broad_family()` helper\n2. Add a real ASJP loader using ISO code matching\n3. Replace `compute_asjp_distance_matrix()` with real NLD computation"}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Let me re-read the current file to get its exact state.\n\nThe file has been modified from what I read. Let me re-read it to get the exact current state."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Good, the file ends at line 864. Now let me make the replacement. I'll replace the entire fabricated ASJP section (lines 793-863) with the real implementation."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Now replace the fabricated ASJP section with the real implementation:"}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Now let me check for linter errors and then test:"}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Good. Now let me test the implementation:"}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"The implementation works well. The results show:\n- **Related languages are closer**: English-German (0.745), French-Spanish (0.715) are the lowest distances\n- **Unrelated languages are distant**: French-Chinese (0.937), German-Swahili (0.932) are the highest\n- **Continuous range**: distances span ~0.71-0.94 with no artificial clustering\n- **All 10 languages found** in the ASJP data\n\nLet me verify the removed constants aren't referenced elsewhere:\n\nThe implementation works. Let me analyze the results:\n\n1. All 10 languages are present - that's good.\n2. Matrix shape is (10, 10) as expected.\n3. The min is 0.0000 (diagonal) and max is 0.9372.\n4. The distances are continuous and not clustered in tiers.\n\nLet me check if the distances make linguistic sense:\n- English <-> German: 0.7451 (closest pair among Germanic, makes sense)\n- French <-> Spanish: 0.7149 (closest pair among Romance, makes sense)\n- Hindi <-> several IE languages: lower than cross-family distances\n\n\n- Chinese <-> Japanese: 0.8602 (not too close - they're different families phonetically, even though they share writing)\n\nThe overall range of 0.7-0.94 seems reasonable for phonetic comparison, likely because ASJP transcription standardizes things and basic vocabulary really does differ significantly across languages. What matters most is whether related languages cluster closer together than unrelated ones, and the data shows that pattern holding - same-branch pairs like English-German and French-Spanish are among the lowest distances, while cross-family pairs are substantially higher.\n\nThe matrix looks solid with the expected symmetry and zero diagonal. Before I mark this complete, I should scan through the codebase to make sure there aren't any other references to the constants I removed."}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Clean. Let me also run a broader test to make sure the module imports correctly and that nothing else is broken:"}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Excellent results! The distances are highly linguistically plausible:\n\n- **Spanish-Portuguese**: 0.51 (very close, as expected for the closest major Romance pair)\n- **French-Italian**: 0.67, **French-Spanish**: 0.71 (Romance cluster nicely ordered)\n- **Russian-Polish**: 0.59 (close Slavic pair)\n- **English-German**: 0.75 (Germanic pair)\n- **Cross-family**: all >0.82 (English-Japanese 0.91, Basque-Arabic 0.88)\n- The full range is 0.47-0.95, continuous and realistic\n- Matrix is symmetric with zero diagonal\n- The fake language `xxx_Fake` was properly filtered out"}]}}
{"role":"assistant","message":{"content":[{"type":"text","text":"Here's a summary of all the changes made to `benchmarks.py`:\n\n**Added:**\n- `import csv` (for CLDF data parsing)\n- `ASJP_CLDF_DIR` constant pointing to the ASJP v20 CLDF data directory\n- `_asjp_cache` module-level dict for in-process caching\n- `_load_asjp_wordlists()` — loads ASJP CLDF `languages.csv` and `forms.csv`, maps ISO 639-3 codes to word lists, picks the most complete doculect for each ISO code, caches results\n- `_levenshtein(s, t)` — standard two-row DP Levenshtein edit distance\n- New `compute_asjp_distance_matrix()` — computes real mean NLD across shared Swadesh concepts using ASJP transcriptions\n\n**Removed:**\n- `_broad_family()` helper\n- `_WITHIN_SUBFAMILY_LDND` hardcoded dictionary\n- `_IE_CLOSE_BRANCHES` hardcoded dictionary\n- `_IE_INTER_BRANCH_DEFAULT` constant\n- `_CROSS_FAMILY_DEFAULT` constant\n- All random noise generation (`rng`, `noise`)\n- The entire fabricated taxonomy-based distance computation\n\n**Key design decisions:**\n- Uses `NLLB_TO_ISO` (ISO 639-3 codes) instead of `NLLB_TO_ASJP` names, since many ASJP Language_IDs in the old mapping were incorrect (e.g., \"GERMAN_ST\" should be \"STANDARD_GERMAN\"). ISO matching via `languages.csv` is robust.\n- For concepts with multiple ASJP transcription variants, the minimum NLD across all variant pairs is used (standard practice).\n- For ISO codes with multiple doculects (e.g., STANDARD_GERMAN vs STANDARD_GERMAN_2), the one with the most complete word list is selected.\n- Data loading takes ~2.5s on first call and is cached thereafter."}]}}