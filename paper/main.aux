\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{dijkstra2002,correia2014,deniz2025}
\citation{nllbteam2022}
\citation{swadesh1952}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{jaeger2018}
\citation{list2018,rzymski2020}
\citation{chang2022}
\citation{pires2019}
\citation{devlin2019}
\citation{chang2022}
\citation{conneau2020}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Multilingual Representation Geometry}{2}{subsection.2.1}\protected@file@percent }
\citation{rajaee2022}
\citation{mu2018}
\citation{voita2019}
\citation{foroutan2022}
\citation{kroll1994,kroll2010}
\citation{dijkstra2002}
\citation{correia2014}
\citation{deniz2025}
\citation{thierry2007}
\citation{malikmoraleda2022}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Cognitive Science of Multilingual Representation}{3}{subsection.2.2}\protected@file@percent }
\citation{swadesh1952}
\citation{jaeger2018}
\citation{berlin1969}
\citation{list2018}
\citation{rzymski2020}
\citation{vulic2020}
\citation{nllbteam2022}
\citation{vaswani2017}
\citation{swadesh1952}
\citation{devlin2019}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{4}{section.3}\protected@file@percent }
\newlabel{sec:methods}{{3}{4}{Methods}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Model and Data}{4}{subsection.3.1}\protected@file@percent }
\citation{mu2018,rajaee2022}
\citation{mu2018}
\citation{jaeger2018}
\citation{list2018}
\citation{rzymski2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Embedding Extraction and Correction}{5}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Experiments}{5}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Swadesh Convergence Ranking.}{5}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Phylogenetic Correlation.}{5}{section*.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Embedding geometry for the concept ``water'' across 29 languages. (a) 3D PCA projection colored by language family shows tight clustering despite orthographic diversity. (b) Pairwise similarity heatmap reveals that same-family languages (e.g., Romance, Slavic) cluster, but cross-family similarity remains high ($>0.93$ for most pairs).}}{6}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:water_manifold}{{1}{6}{Embedding geometry for the concept ``water'' across 29 languages. (a) 3D PCA projection colored by language family shows tight clustering despite orthographic diversity. (b) Pairwise similarity heatmap reveals that same-family languages (e.g., Romance, Slavic) cluster, but cross-family similarity remains high ($>0.93$ for most pairs)}{figure.caption.1}{}}
\citation{correia2014}
\citation{berlin1969}
\citation{mikolov2013}
\citation{chang2022}
\@writefile{toc}{\contentsline {paragraph}{Colexification Proximity.}{7}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conceptual Store Metric.}{7}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Color Circle.}{7}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Offset Invariance.}{7}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Isotropy Correction Validation.}{7}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{7}{section.4}\protected@file@percent }
\newlabel{sec:results}{{4}{7}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Illustrative Example: Water}{7}{subsection.4.1}\protected@file@percent }
\citation{swadesh1952}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Swadesh Core Vocabulary Convergence}{8}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Swadesh convergence ranking vs.\ surface-form similarity. (a) Orthographic similarity (normalized Levenshtein distance on Latin-script word forms, $R^2 = 0.012{}$) and (b) phonological similarity (after crude phonetic normalization, $R^2 = 0.004{}$) plotted against embedding convergence (isotropy-corrected). Points are colored by semantic category. Neither measure predicts convergence: over 98\% of the convergence signal is attributable to semantic rather than surface-form factors. Concepts in the upper-left quadrant converge strongly in embedding space \emph  {despite} low surface-form similarity---the strongest candidates for genuine conceptual universals.}}{8}{figure.caption.9}\protected@file@percent }
\newlabel{fig:swadesh}{{2}{8}{Swadesh convergence ranking vs.\ surface-form similarity. (a) Orthographic similarity (normalized Levenshtein distance on Latin-script word forms, $R^2 = \DecompRsqOrtho {}$) and (b) phonological similarity (after crude phonetic normalization, $R^2 = \DecompRsqPhon {}$) plotted against embedding convergence (isotropy-corrected). Points are colored by semantic category. Neither measure predicts convergence: over 98\% of the convergence signal is attributable to semantic rather than surface-form factors. Concepts in the upper-left quadrant converge strongly in embedding space \emph {despite} low surface-form similarity---the strongest candidates for genuine conceptual universals}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Swadesh vs.\ Non-Swadesh Vocabulary}{8}{subsection.4.3}\protected@file@percent }
\citation{miller1995}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Category Summary}{9}{subsection.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Convergence by semantic category. Violin plots with individual data points for each Swadesh semantic category (isotropy-corrected). The dashed line marks the overall mean. Nature and People categories converge most strongly; Pronouns converge least, consistent with their high cross-linguistic grammaticalization variability.}}{9}{figure.caption.10}\protected@file@percent }
\newlabel{fig:category_summary}{{3}{9}{Convergence by semantic category. Violin plots with individual data points for each Swadesh semantic category (isotropy-corrected). The dashed line marks the overall mean. Nature and People categories converge most strongly; Pronouns converge least, consistent with their high cross-linguistic grammaticalization variability}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {paragraph}{Polysemy confound.}{9}{section*.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Per-concept convergence scores grouped by semantic category (sorted by category mean, highest at top). Each dot is one Swadesh concept; the dashed line marks the overall mean. Shaded bands delineate category boundaries, enabling identification of within-category outliers such as polysemous items that depress their category's aggregate score.}}{10}{figure.caption.11}\protected@file@percent }
\newlabel{fig:category_detail}{{4}{10}{Per-concept convergence scores grouped by semantic category (sorted by category mean, highest at top). Each dot is one Swadesh concept; the dashed line marks the overall mean. Shaded bands delineate category boundaries, enabling identification of within-category outliers such as polysemous items that depress their category's aggregate score}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Isotropy Correction Validation}{11}{subsection.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Isotropy correction validation. (a) Scatter of raw vs.\ corrected convergence scores (Spearman $\rho = 0.990{}$), colored by semantic category; points below the diagonal indicate concepts whose convergence decreased after correction. (b) Top-10 and bottom-10 concepts under each regime; the overlap is substantial, with a few concepts reranked. (c) Sensitivity of the convergence ranking to the ABTT hyperparameter $k$: all pairwise Spearman correlations with the reference $k=3$ ranking span 0.98--1.00{}, confirming robustness.}}{11}{figure.caption.13}\protected@file@percent }
\newlabel{fig:isotropy_validation}{{5}{11}{Isotropy correction validation. (a) Scatter of raw vs.\ corrected convergence scores (Spearman $\rho = \IsotropySpearmanRho {}$), colored by semantic category; points below the diagonal indicate concepts whose convergence decreased after correction. (b) Top-10 and bottom-10 concepts under each regime; the overlap is substantial, with a few concepts reranked. (c) Sensitivity of the convergence ranking to the ABTT hyperparameter $k$: all pairwise Spearman correlations with the reference $k=3$ ranking span \IsotropyKRange {}, confirming robustness}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Carrier Sentence Robustness}{11}{subsection.4.6}\protected@file@percent }
\citation{tenney2019}
\citation{jaeger2018}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Carrier sentence robustness analysis. (a) Scatter of contextualized vs.\ decontextualized convergence scores (Spearman $\rho = 0.867{}$). Points near the identity line indicate concepts whose convergence is insensitive to the carrier sentence. (b) Slopegraph of the top-20 concepts under each condition, showing minimal reranking.}}{12}{figure.caption.14}\protected@file@percent }
\newlabel{fig:carrier_baseline}{{6}{12}{Carrier sentence robustness analysis. (a) Scatter of contextualized vs.\ decontextualized convergence scores (Spearman $\rho = \CarrierBaselineRho {}$). Points near the identity line indicate concepts whose convergence is insensitive to the carrier sentence. (b) Slopegraph of the top-20 concepts under each condition, showing minimal reranking}{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Layer-wise Emergence of Semantic Structure}{12}{subsection.4.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Layer-wise emergence of language-universal semantic structure across 12{} encoder layers (computed on a 39{}-language subset). (a) Mean Swadesh convergence increases monotonically, with a sharp rise around layer~1{}, paralleling the ``NLP pipeline'' effect. (b) The Conceptual Store Metric (both raw and mean-centered ratios) shows a similar trajectory, with the centered ratio exhibiting a phase transition at layer~6{}. (c) Per-concept convergence heatmap reveals that concrete, perceptually grounded concepts (top rows) achieve high cross-lingual convergence earlier than abstract or polysemous concepts (bottom rows).}}{13}{figure.caption.15}\protected@file@percent }
\newlabel{fig:layerwise}{{7}{13}{Layer-wise emergence of language-universal semantic structure across \LayerwiseNumLayers {} encoder layers (computed on a \LayerwiseNumLangs {}-language subset). (a) Mean Swadesh convergence increases monotonically, with a sharp rise around layer~\LayerwiseEmergenceLayer {}, paralleling the ``NLP pipeline'' effect. (b) The Conceptual Store Metric (both raw and mean-centered ratios) shows a similar trajectory, with the centered ratio exhibiting a phase transition at layer~\LayerwisePhaseTrans {}. (c) Per-concept convergence heatmap reveals that concrete, perceptually grounded concepts (top rows) achieve high cross-lingual convergence earlier than abstract or polysemous concepts (bottom rows)}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8}Phylogenetic Distance Correlation}{13}{subsection.4.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Phylogenetic structure in the NLLB-200 embedding space. Left: heatmap of pairwise embedding coherence (normalized to $[0,1]$; white = $0$, red = $1$) between 88{} languages, ordered by hierarchical clustering. Right: dendrogram derived from the embedding distance matrix. Major language families (e.g., Indo-European, Austronesian, Niger-Congo) form recognizable clusters.}}{14}{figure.caption.16}\protected@file@percent }
\newlabel{fig:phylo}{{8}{14}{Phylogenetic structure in the NLLB-200 embedding space. Left: heatmap of pairwise embedding coherence (normalized to $[0,1]$; white = $0$, red = $1$) between \MantelNumLangs {} languages, ordered by hierarchical clustering. Right: dendrogram derived from the embedding distance matrix. Major language families (e.g., Indo-European, Austronesian, Niger-Congo) form recognizable clusters}{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Mantel test scatter: pairwise embedding distance vs.\ ASJP phonetic distance across 88{} languages, colored by phylogenetic relationship tier---same subfamily (blue), cross-branch Indo-European (amber), and cross-family (red). Dashed lines show per-group linear fits with Spearman $\rho $ values. The overall Mantel $\rho = 0.13{}$ ($p = 0.020{}$).}}{15}{figure.caption.17}\protected@file@percent }
\newlabel{fig:mantel_scatter}{{9}{15}{Mantel test scatter: pairwise embedding distance vs.\ ASJP phonetic distance across \MantelNumLangs {} languages, colored by phylogenetic relationship tier---same subfamily (blue), cross-branch Indo-European (amber), and cross-family (red). Dashed lines show per-group linear fits with Spearman $\rho $ values. The overall Mantel $\rho = \MantelRho {}$ ($p = \MantelP {}$)}{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces 2D PCA projection of Swadesh concept embeddings pooled across all 19 language families. Small translucent dots show per-family centroid positions (one per concept per family, ${\sim }1{,}900$ points total); convex hulls delineate each semantic category's spread; large opaque dots mark the overall cross-lingual centroids. Body parts and nature terms occupy distinct regions of the space, while pronouns cluster tightly---confirming that the model's geometry is organized by meaning rather than arbitrary lexical associations.}}{16}{figure.caption.18}\protected@file@percent }
\newlabel{fig:concept_map}{{10}{16}{2D PCA projection of Swadesh concept embeddings pooled across all 19 language families. Small translucent dots show per-family centroid positions (one per concept per family, ${\sim }1{,}900$ points total); convex hulls delineate each semantic category's spread; large opaque dots mark the overall cross-lingual centroids. Body parts and nature terms occupy distinct regions of the space, while pronouns cluster tightly---confirming that the model's geometry is organized by meaning rather than arbitrary lexical associations}{figure.caption.18}{}}
\citation{list2018,rzymski2020}
\citation{list2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9}Colexification Proximity}{17}{subsection.4.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Cosine similarity as a function of colexification frequency for 1431{} Swadesh concept pairs. Each red point is a pair attested by at least one CLICS\textsuperscript  {3} language family; grey points are non-colexified controls. The dashed line is a linear fit. Spearman $\rho _s = 0.17{}$ ($p = 2.15e-10{}$). Selected pairs are labelled to illustrate the semantic content at different frequency levels.}}{17}{figure.caption.19}\protected@file@percent }
\newlabel{fig:colex}{{11}{17}{Cosine similarity as a function of colexification frequency for \ColexNumPairs {} Swadesh concept pairs. Each red point is a pair attested by at least one CLICS\textsuperscript {3} language family; grey points are non-colexified controls. The dashed line is a linear fit. Spearman $\rho _s = \ColexSpearmanRho {}$ ($p = \ColexSpearmanP {}$). Selected pairs are labelled to illustrate the semantic content at different frequency levels}{figure.caption.19}{}}
\citation{correia2014}
\citation{berlin1969}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10}Conceptual Store Metric}{18}{subsection.4.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11}Color Circle}{18}{subsection.4.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces PCA projection of 11{} Berlin \& Kay basic color terms across 136{} languages. (a) 2D chromatic plane: small translucent dots show per-language embeddings; convex hulls delineate spread; large circles mark cross-lingual centroids. Warm and cool colors occupy opposing regions, recovering the circular topology of perceptual color space. (b) 3D view: the third principal component separates achromatic terms (white, black, grey; square markers) from the chromatic plane, revealing a luminance axis orthogonal to the hue circle---consistent with the achromatic--chromatic distinction in the Berlin \& Kay hierarchy.}}{18}{figure.caption.20}\protected@file@percent }
\newlabel{fig:color}{{12}{18}{PCA projection of \ColorNumColors {} Berlin \& Kay basic color terms across \ColorNumLanguages {} languages. (a) 2D chromatic plane: small translucent dots show per-language embeddings; convex hulls delineate spread; large circles mark cross-lingual centroids. Warm and cool colors occupy opposing regions, recovering the circular topology of perceptual color space. (b) 3D view: the third principal component separates achromatic terms (white, black, grey; square markers) from the chromatic plane, revealing a luminance axis orthogonal to the hue circle---consistent with the achromatic--chromatic distinction in the Berlin \& Kay hierarchy}{figure.caption.20}{}}
\citation{mikolov2013}
\citation{chang2022}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12}Semantic Offset Invariance}{19}{subsection.4.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Semantic offset invariance across languages. (a) Each bar shows the mean cosine similarity between per-language offset vectors and the centroid offset for a given concept pair; the best-performing pair is \textit  {fire--water{}}. (b) Per-family disaggregation: each cell shows the mean consistency averaged over languages within each family. Rows are concept pairs (sorted by overall consistency); columns are language families. Warmer colors indicate higher consistency.}}{19}{figure.caption.21}\protected@file@percent }
\newlabel{fig:offset}{{13}{19}{Semantic offset invariance across languages. (a) Each bar shows the mean cosine similarity between per-language offset vectors and the centroid offset for a given concept pair; the best-performing pair is \textit {\OffsetBestPair {}}. (b) Per-family disaggregation: each cell shows the mean consistency averaged over languages within each family. Rows are concept pairs (sorted by overall consistency); columns are language families. Warmer colors indicate higher consistency}{figure.caption.21}{}}
\citation{correia2014}
\citation{deniz2025}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Offset vector demonstration for the top-4 concept pairs. (a) Joint PCA projection showing per-language embeddings (translucent), centroid positions (white markers), and offset arrows for each pair (colored). Thin arrows show individual per-language offsets; bold arrows show centroid offsets. (b) All offset vectors plotted from a common origin, revealing directional consistency: per-language offsets cluster tightly around their centroid direction for each pair, confirming language-invariant relational structure.}}{20}{figure.caption.22}\protected@file@percent }
\newlabel{fig:offset_vector_demo}{{14}{20}{Offset vector demonstration for the top-4 concept pairs. (a) Joint PCA projection showing per-language embeddings (translucent), centroid positions (white markers), and offset arrows for each pair (colored). Thin arrows show individual per-language offsets; bold arrows show centroid offsets. (b) All offset vectors plotted from a common origin, revealing directional consistency: per-language offsets cluster tightly around their centroid direction for each pair, confirming language-invariant relational structure}{figure.caption.22}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{20}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Structural Parallels with Cognitive Models}{20}{subsection.5.1}\protected@file@percent }
\citation{dijkstra2002}
\citation{kroll2010}
\citation{mikolov2013}
\citation{correia2014}
\citation{voita2019}
\citation{tenney2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Limitations}{21}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Carrier sentence confound.}{21}{section*.23}\protected@file@percent }
\citation{correia2014}
\citation{nllbteam2022}
\citation{thierry2007}
\@writefile{toc}{\contentsline {paragraph}{Conceptual store improvement.}{22}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Tokenization artifacts.}{22}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Raw cosine unreliable.}{22}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Non-Swadesh selection bias.}{22}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ASJP coverage.}{22}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Additional limitations.}{22}{section*.29}\protected@file@percent }
\citation{list2018}
\citation{malikmoraleda2022}
\citation{correia2014}
\citation{deniz2025}
\citation{hewitt2019}
\citation{voita2019,clark2019}
\citation{kroll2010}
\citation{chang2022}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Broader Implications}{23}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Future Work}{23}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computational ATL layer.}{23}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Per-head cross-attention decomposition.}{23}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{RHM asymmetry.}{23}{section*.32}\protected@file@percent }
\citation{dijkstra2002}
\citation{kroll2010}
\citation{correia2014}
\citation{voita2019}
\citation{conneau2020}
\citation{devlin2019}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{24}{section.6}\protected@file@percent }
\bibstyle{plainnat}
\bibdata{references}
\bibcite{berlin1969}{{1}{1969}{{Berlin and Kay}}{{}}}
\bibcite{chang2022}{{2}{2022}{{Chang et~al.}}{{Chang, Tu, and Bergen}}}
\bibcite{deniz2025}{{3}{2025}{{Chen et~al.}}{{Chen, Gong, Tseng, Gallant, Klein, and Deniz}}}
\bibcite{clark2019}{{4}{2019}{{Clark et~al.}}{{Clark, Khandelwal, Levy, and Manning}}}
\bibcite{conneau2020}{{5}{2020}{{Conneau et~al.}}{{Conneau, Khandelwal, Goyal, Chaudhary, Wenzek, Guzm{\'a}n, Grave, Ott, Zettlemoyer, and Stoyanov}}}
\bibcite{correia2014}{{6}{2014}{{Correia et~al.}}{{Correia, Jansma, Bonte, and Hausfeld}}}
\bibcite{devlin2019}{{7}{2019}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{dijkstra2002}{{8}{2002}{{Dijkstra and van Heuven}}{{}}}
\bibcite{foroutan2022}{{9}{2022}{{Foroutan et~al.}}{{Foroutan, Banaei, Aberer, and Bosselut}}}
\bibcite{hewitt2019}{{10}{2019}{{Hewitt and Manning}}{{}}}
\bibcite{jaeger2018}{{11}{2018}{{J{\"a}ger}}{{}}}
\bibcite{kroll1994}{{12}{1994}{{Kroll and Stewart}}{{}}}
\bibcite{kroll2010}{{13}{2010}{{Kroll et~al.}}{{Kroll, van Hell, Tokowicz, and Green}}}
\bibcite{list2018}{{14}{2018}{{List et~al.}}{{List, Greenhill, Anderson, Mayer, Tresoldi, and Forkel}}}
\bibcite{malikmoraleda2022}{{15}{2022}{{Malik-Moraleda et~al.}}{{Malik-Moraleda, Ayyash, Gall{\'e}e, Affourtit, Hoffmann, Mineroff, Jouravlev, and Fedorenko}}}
\bibcite{mikolov2013}{{16}{2013}{{Mikolov et~al.}}{{Mikolov, Chen, Corrado, and Dean}}}
\bibcite{miller1995}{{17}{1995}{{Miller}}{{}}}
\bibcite{mu2018}{{18}{2018}{{Mu and Viswanath}}{{}}}
\bibcite{nllbteam2022}{{19}{2022}{{{NLLB Team} et~al.}}{{{NLLB Team}, Costa-juss{\`a}, Cross, {\c {C}}elebi, Elbayad, Heafield, Heffernan, Kalbassi, Lam, Licht, Maillard, Sun, Wang, Wenzek, Youngblood, Akula, Baez-Yates, Barber, Buj, Buzber, Chaudhary, et~al.}}}
\bibcite{pires2019}{{20}{2019}{{Pires et~al.}}{{Pires, Schlinger, and Garrette}}}
\bibcite{rajaee2022}{{21}{2022}{{Rajaee and Pilehvar}}{{}}}
\bibcite{rzymski2020}{{22}{2020}{{Rzymski et~al.}}{{Rzymski, Tresoldi, Greenhill, Wu, Schweikhard, Finley, Cysouw, Forkel, and List}}}
\bibcite{swadesh1952}{{23}{1952}{{Swadesh}}{{}}}
\bibcite{tenney2019}{{24}{2019}{{Tenney et~al.}}{{Tenney, Das, and Pavlick}}}
\bibcite{thierry2007}{{25}{2007}{{Thierry and Wu}}{{}}}
\bibcite{vaswani2017}{{26}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{voita2019}{{27}{2019}{{Voita et~al.}}{{Voita, Talbot, Moiseev, Sennrich, and Titov}}}
\bibcite{vulic2020}{{28}{2020}{{Vuli{\'c} et~al.}}{{Vuli{\'c}, Baker, Ponti, Peti-Stanti{\'c}, Reichart, and Korhonen}}}
\gdef \@abspage@last{27}
