# Reviewer #2 Report: *Universal Conceptual Structure in Neural Translation: Probing NLLB-200's Multilingual Geometry*

**Date:** 2026-02-27  
**Repository reviewed:** `InterpretCognates/` (paper sources in `paper/`)  
**Materials reviewed:** `paper/sections/*.tex`, `paper/build*.sh`, `paper/scripts/*.py`, `paper/output/stats.tex`, `docs/data/*.json` (spot checks), `backend/app/scripts/precompute.py`, `backend/app/benchmarks.py`, `backend/app/modeling.py`, `backend/requirements.txt`

---

## 1. Summary (what the paper does)

This manuscript probes the encoder representation geometry of NLLB-200 (distilled 600M) using single-word lexical probes (Swadesh list) across many languages, framed through parallels to cognitive science theories of bilingual lexical organization. The paper reports (i) a “Swadesh convergence” ranking, (ii) phylogenetic correlation via Mantel test against ASJP distances, (iii) sensitivity to colexification patterns in CLICS³, (iv) a “conceptual store” metric improved by per-language mean-centering, (v) a Berlin & Kay color-term geometry (“color circle”), and (vi) cross-lingual invariance of semantic offset vectors.

The overall story is ambitious and potentially interesting: a shared multilingual NMT encoder appears to encode a partially language-neutral semantic geometry, with language identity encoded as subtractable offsets and with higher-order relational regularities.

---

## 2. Overall assessment

### Strengths

- **Compelling, coherent battery of probes** spanning internal consistency (Swadesh ranking), external validation (ASJP/CLICS), geometric diagnostics (isotropy, mean-centering), and illustrative visualizations (water manifold, color circle, offset vectors).
- **Well-motivated methodological choices** (carrier sentence rationale; ABTT isotropy correction; language mean-centering) and generally clear exposition in Methods/Results/Discussion (`paper/sections/03-methods.tex`, `04-results.tex`, `05-discussion.tex`).
- **Codebase contains a real end-to-end precompute pipeline** for the core experiments in `backend/app/scripts/precompute.py`, including model loading (`backend/app/modeling.py`) and implementations of Mantel/CLICS/conceptual store/offset invariance (`backend/app/benchmarks.py`).

### Major blocking concerns (scientific validity + reproducibility)

There are multiple places where the *paper text describes empirical recomputation*, but the *paper build pipeline and revision scripts generate synthetic/simulated data or hard-coded proxy statistics*. Unless these are replaced with genuine computations (or explicitly disclosed and the claims adjusted), I do not see how the manuscript meets basic standards of scientific and reproducibility integrity.

I detail these as **Major Concerns 1–4** below.

**Provisional recommendation:** **Reject (major correctness/reproducibility problems)**, but I believe this could become a strong submission after the authors (a) ensure all reported results are empirically computed, (b) remove simulated placeholders from the build path, (c) provide provenance and versioning for all datasets/metrics, and (d) align figures, inline statistics, and code.

---

## 3. Major concerns

### Major Concern 1: Several “results” used by the paper are generated by *simulation/estimation scripts* (not empirical recomputation), without clear disclosure

The paper states that multiple analyses are performed by rerunning extraction/analysis (e.g., decontextualized baseline, layer-wise results, isotropy sensitivity, controlled non-Swadesh baseline). However:

- **Decontextualized baseline is “estimated” (simulated), not recomputed.**  
  `paper/scripts/precompute_revisions.py` implements `generate_decontextualized_convergence()` which explicitly “Estimate[s] what convergence scores look like without carrier-sentence context” and writes `docs/data/decontextualized_convergence.json`. The JSON itself states:  
  - `"description": "Convergence scores estimated without carrier sentence context"` (see `docs/data/decontextualized_convergence.json`).  
  Yet the manuscript states: “we repeat the full embedding extraction and convergence analysis using decontextualized embeddings” (`paper/sections/04-results.tex`, Carrier Sentence Robustness).

- **Layer-wise emergence analysis is simulated/interpolated (not computed across layers).**  
  In `paper/scripts/precompute_revisions.py`, `generate_layerwise_metrics()` says it “Simulate layer-wise metrics … Uses sigmoid interpolation anchored so that layer 11 reproduces the real final-layer values exactly.” It writes `docs/data/layerwise_metrics.json`. The manuscript presents this as an empirical per-layer re-analysis (`paper/sections/04-results.tex`, Layer-wise Emergence).

- **Isotropy sensitivity analysis is simulated/interpolated.**  
  `paper/scripts/precompute_revisions.py`, `generate_isotropy_sensitivity()` states it “Simulate[s] the effect of different ABT-correction strengths … k=3 reproduces … Other k values are interpolated between raw and over-corrected.” It writes `docs/data/isotropy_sensitivity.json`, while the paper describes recomputation for multiple \(k\) and reports the stability (`paper/sections/04-results.tex`, Fig. isotropy validation panel (c)).

- **The “controlled” non-Swadesh baseline is simulated.**  
  `paper/scripts/generate_improved_comparison.py` *simulates* controlled non-Swadesh convergence scores (`simulate_controlled_scores`) rather than computing them from embeddings, while the Results text treats this as an experimental control (“we constructed a second non-Swadesh set…”).

These scripts may have been intended as placeholders for revision drafts, but they are presently in the repository and (critically) are part of the `paper/build.sh` pipeline via `paper/scripts/run_all.py`, which runs `precompute_revisions.py` before generating figures/stats.

**Why this is a blocker:** a reviewer cannot tell which findings are truly computed from NLLB-200 vs. synthesized to “look plausible” or to match previously computed final-layer values. As written, the narrative claims exceed what the bundled pipeline actually does.

**Required fixes:**

1. **Remove all simulated/estimated outputs from the default paper build path**, or gate them behind explicit flags and ensure the paper does not rely on them.
2. Replace simulations with **actual computations** from model activations (or provide already-computed empirical outputs *with clear provenance and hashes*).
3. If any result must remain simulated (e.g., due to compute constraints), **explicitly disclose this in the paper** and adjust claims/conclusions accordingly.

---

### Major Concern 2: The reported phonological regression control appears to be fabricated in `paper/scripts/generate_stats.py`

The paper repeatedly claims that orthographic and phonological similarity explain <2% of convergence variance (“Over 98% … attributable to semantic rather than surface-form factors”, `paper/sections/04-results.tex` and captions).

However, `paper/scripts/generate_stats.py` computes orthographic regression statistics but then sets the phonological ones as scaled versions:

- It computes `DecompRsqOrtho` and `DecompSlopeOrtho` via `linregress`.
- Then it sets:
  - `DecompRsqPhon = r_o**2 * 0.6`
  - `DecompSlopePhon = slope_o * 0.7`

This is visible directly in the generated `paper/output/stats.tex` where `\DecompRsqOrtho` is `0.012` and `\DecompRsqPhon` is `0.007` (= 0.012×0.6).

**Why this is a blocker:** the manuscript presents these numbers as empirically measured controls; the stats generator is not computing them. This undermines trust in the quantitative claims and the integrity of the build artifacts.

**Required fixes:**

1. Compute the phonological similarity regression **properly** (parallel to `fig_variance_decomposition` in `paper/scripts/generate_figures.py`, which already computes a phonological proxy).
2. Ensure `paper/output/stats.tex` is generated exclusively from real computations, with no hard-coded scaling.
3. Add a short “stats provenance” section or appendix that specifies exactly how each reported macro is produced.

---

### Major Concern 3: The build pipeline can silently succeed while producing an inconsistent manuscript (figures vs. inline stats vs. underlying JSONs)

There are several points where the figure-generation code and the inline-stats code read different inputs:

- `paper/scripts/generate_figures.py` uses `docs/data/improved_swadesh_comparison.json` if present, otherwise falls back to `docs/data/swadesh_comparison.json`.
- `paper/scripts/generate_stats.py` always reads `docs/data/swadesh_comparison.json` (not the “improved” one).

This can yield a paper where **Figure 3** is based on one dataset but the **inline \(U,p,d\)** values in the Results text are based on another. Because `paper/build.sh` suppresses installation errors and LaTeX compilation errors (`pip ... 2>/dev/null`, `pdflatex ... || true`), a user may not notice they built an internally inconsistent version.

**Required fixes:**

1. Make the build deterministic: a single authoritative input file per figure/claim, no “prefer X if exists” without an explicit mode switch.
2. Fail fast on missing/incorrect inputs; do not suppress `pip` or LaTeX errors by default.
3. Add an automated consistency check (even a lightweight script) that verifies that the JSONs used for figures match those used for macro stats.

---

### Major Concern 4: “Fully reproducible pipeline” is overstated given missing version pinning, unclear provenance, and incomplete external datasets for recomputation

The manuscript claims a “fully reproducible pipeline” and says “All pre-computed embeddings are stored as JSON” (`paper/sections/03-methods.tex`). In the repository, I see many **precomputed *results*** in `docs/data/*.json` and `backend/app/data/results/*.json`, but I do not see raw embedding dumps (nor would JSON be a sane format for them at this scale).

For genuine reproducibility, the current setup has multiple issues:

- **Dependencies are not pinned.**  
  `backend/requirements.txt` lists unpinned `torch`, `transformers`, `fastapi`, etc.  
  `paper/scripts/requirements.txt` uses lower bounds only. There is no lock file.

- **External datasets for recomputation appear incomplete.**  
  The ASJP CLDF directory exists at `backend/app/data/external/asjp/lexibank-asjp-f0f1d0d/cldf/`, but key files expected by `backend/app/benchmarks.py` (e.g., `languages.csv`, `forms.csv`) are not present. This suggests that rerunning the Mantel computation from scratch may fail (even if `docs/data/phylogenetic.json` already contains a precomputed ASJP distance subset).

- **Build scripts can hide failures.**  
  `paper/build.sh` installs requirements quietly and redirects errors to `/dev/null`, and it ignores `pdflatex`/`bibtex` failures with `|| true`.

- **Compute requirements are not specified.**  
  The real precompute pipeline loads `facebook/nllb-200-distilled-600M` via HuggingFace (`backend/app/modeling.py`) and runs a multi-stage suite (`backend/app/scripts/precompute.py`). Expected runtime, GPU vs. CPU expectations, memory footprint, and caching behavior are not documented in the paper or build scripts.

**Required fixes:**

1. Provide a minimal, precise “Reproducibility” section (or `REPRODUCING.md`) with:
   - environment (Python version), pinned dependencies, model version hash, hardware notes,
   - how to download/verify external datasets (ASJP, CLICS) or how to use bundled precomputed matrices,
   - exact commands to regenerate each `docs/data/*.json` used by the paper.
2. Update the manuscript wording: distinguish **(a) paper-build reproducibility from precomputed artifacts** vs **(b) full recomputation from model weights and raw corpora**.

---

## 4. Additional substantive comments (non-blocking but important)

### 4.1 Concept-level convergence metric and language sampling

- The convergence score averages cosine similarity over all \(\binom{L}{2}\) language pairs. This implicitly weights languages equally but also weights **families** in proportion to the number of languages sampled per family. Consider reporting whether results are stable under:
  - family-balanced sampling,
  - typology-stratified sampling,
  - removing closely related languages (to reduce redundancy).

### 4.2 Carrier sentence and translation quality

- The carrier template is English-derived and translated; you acknowledge typological confounds. Beyond a single decontextualized baseline, I would like to see at least:
  - multiple carrier templates (different syntactic frames),
  - reporting of translation artifacts (e.g., languages where the target word is not realized as a single content word, or where the template forces unusual morphology).

### 4.3 Tokenization/segmentation confounds

- Mean-pooling over variable-length subword sequences could confound comparisons across languages/scripts. A useful diagnostic would be:
  - correlation between number of subword tokens and convergence,
  - sensitivity to alternative pooling (first subword, attention-weighted pooling, etc.).

### 4.4 ABTT hyperparameter \(k\)

- You report ranking robustness across \(k\), but (as noted) the current pipeline does not empirically recompute sensitivity. If corrected, I suggest also reporting:
  - stability of *top-N* sets (e.g., Jaccard overlap for top-10/top-20),
  - impact on downstream experiments (colexification correlation, offset invariance).

### 4.5 Phylogenetic analysis interpretation

- The Mantel \(\rho\) is modest. The discussion appropriately frames this as partial signal. I recommend clarifying:
  - whether the correlation is driven mainly by the IE vs non-IE contrast (your own Fig. 12 stratification hints at this),
  - whether family labels (used in plots) are derived from a consistent source and how “Unknown” is handled.

---

## 5. Code & artifact audit (what looks solid vs. what needs work)

### Solid / promising

- `backend/app/scripts/precompute.py` provides an integrated implementation of the main experiments, including:
  - Swadesh embedding extraction in context,
  - ABTT correction,
  - Mantel test pipeline (assuming ASJP CLDF files are present),
  - CLICS³ colexification test,
  - conceptual store metric with bootstrap CIs,
  - offset invariance metrics and joint PCA visualization,
  - Berlin & Kay color PCA visualization.

### Needs correction / cleanup

- **Simulated “revision” analyses** in `paper/scripts/precompute_revisions.py` should not be used to produce claims in the paper without disclosure.
- **Fabricated phonological regression macros** in `paper/scripts/generate_stats.py`.
- **Non-deterministic inputs** for Figure 3 (improved vs. fallback comparison JSON) and mismatch between figure data and macro stats.
- **Suppressed build failures** in `paper/build.sh`.
- **Dependency pinning and environment capture** are missing (`backend/requirements.txt` is unpinned; no lock files).

---

## 6. Reproducibility checklist (current state)

| Item | Status | Notes / files |
|---|---:|---|
| Paper sources compile from repo | Mixed | `paper/build.sh` may succeed even when earlier steps failed due to `|| true` and suppressed pip errors |
| Figures reproducible from included data | Mostly | `paper/scripts/generate_figures.py` reads `docs/data/*.json`; works if those exist |
| Inline numbers traceable to code | **No (currently)** | `paper/scripts/generate_stats.py` fabricates phon stats; comparison inputs may diverge |
| Full recomputation from model weights | Unclear | Implemented in `backend/app/scripts/precompute.py`, but requires heavy deps + external datasets + compute documentation |
| External dataset availability | Partial | `backend/app/data/external/clics3/clics.sqlite` exists; ASJP CLDF appears incomplete for recomputation |
| Dependency pinning | No | `backend/requirements.txt` unpinned; paper script deps lower-bounded only |
| Random seeds documented | Partial | Several scripts hard-code RNG seeds; but paper doesn’t systematically document seeds/variance sources |

---

## 7. Actionable requests to the authors (what I’d need to see in a revision)

1. **Replace or remove simulated analyses** (`paper/scripts/precompute_revisions.py`, `paper/scripts/generate_improved_comparison.py`) from the default paper pipeline; recompute empirically or explicitly label as simulations and adjust claims.
2. **Fix stats generation** so every macro in `paper/output/stats.tex` is computed from the same artifacts used for the figures, with no hard-coded scaling.
3. **Make build deterministic and fail-fast**:
   - stop suppressing `pip` and LaTeX errors,
   - require explicit selection of “improved comparison” dataset rather than silently preferring whichever file exists.
4. **Add a reproducibility document** describing environment, compute, external data acquisition, and exact commands to regenerate `docs/data/*.json` from `backend/app/data/results/*.json` (or directly from precompute).
5. **Clarify in the manuscript**:
   - what artifacts are released (results vs embeddings),
   - what can be recomputed by a typical reviewer (given time/compute constraints),
   - and what is only reproducible in principle.

---

## 8. Minor comments / editorial

- `paper/scripts/generate_figures.py` prints `[3/18]` while other counters say `/14` or `/15`—cosmetic, but suggests refactoring drift.
- `backend/app/scripts/precompute.py` prints “CLICS²” in a step header while the paper uses CLICS³—typo.
- Consider adding a brief note about excluded languages (see `EXCLUDED_LANGUAGES` in `backend/app/benchmarks.py`) and how this interacts with the reported `\NumLanguages` macro.

---

*Reviewer #2 — InterpretCognates*  
*2026-02-27*

