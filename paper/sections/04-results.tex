\section{Results}
\label{sec:results}

We present our six core experiments alongside descriptive illustrations and validation analyses, organized along a progression from broad distributional patterns through external validation to geometric tests of cognitive hypotheses.

\subsection{Illustrative Example: Water}

Before presenting the full ranking, we illustrate the geometry of a single concept.
Figure~\ref{fig:water_manifold} shows the 29-language embedding manifold for ``water'' --- a concept with diverse surface forms across language families (\textit{agua}, \textit{eau}, \textit{Wasser}, \textit{maji}, etc.).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_water_manifold.pdf}
  \caption{Embedding geometry for the concept ``water'' across 29 languages. (a) 3D PCA projection colored by language family shows tight clustering despite orthographic diversity. (b) Pairwise similarity heatmap reveals that same-family languages (e.g., Romance, Slavic) cluster, but cross-family similarity remains high ($>0.93$ for most pairs).}
  \label{fig:water_manifold}
\end{figure}

Despite radically different surface forms and scripts, the embeddings cluster tightly in representation space: same-family languages form sub-clusters, but cross-family similarity remains high, suggesting that the model maps ``water'' to a shared semantic region regardless of its lexical realization.
This example motivates the systematic analysis that follows.

\subsection{Swadesh Core Vocabulary Convergence}

Across the \NumConcepts{} Swadesh items embedded in \NumLanguages{} languages, the mean cross-lingual convergence score---defined as the average pairwise cosine similarity over all language pairs for a given concept---is \SwadeshMean{} ($\sigma = \SwadeshStd{}$), with individual concepts ranging from \SwadeshMin{} to \SwadeshMax{}.
Figure~\ref{fig:swadesh} presents the full ranking.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth,height=0.85\textheight,keepaspectratio]{figures/fig_swadesh_ranking.pdf}
  \caption{Swadesh convergence ranking. Each bar shows the mean pairwise cosine similarity for a concept across all \NumLanguages{} language pairs. Concepts at the top of the ranking are represented most consistently across languages in the NLLB-200 encoder space.}
  \label{fig:swadesh}
\end{figure}

The highest-ranked concept is \textit{\SwadeshTopConcept{}}, while the lowest is \textit{\SwadeshBottomConcept{}}.
The distribution reveals a clear pattern: concepts that are concrete, perceptually grounded, and monosemous (e.g., body parts, celestial objects, kinship terms) tend to cluster near the top, whereas concepts that are abstract or polysemous tend to occupy the bottom ranks.
Several of the lowest-scoring items---such as \textit{bark} (tree covering vs.\ the sound a dog makes) and \textit{lie} (recline vs.\ falsehood)---are well-known cases of systematic polysemy in English that do not transfer to other languages, resulting in dispersed cross-lingual representations.
This ordering is broadly consistent with the intuition behind the Swadesh list itself: the most culturally stable meanings \citep{swadesh1952} are also those that the model encodes most uniformly.

\subsection{Swadesh vs.\ Non-Swadesh Vocabulary}

To test whether the Swadesh convergence pattern reflects cultural stability rather than incidental properties of the vocabulary, we compared convergence scores for \SwadeshCompNumSwadesh{} Swadesh concepts against \SwadeshCompNumNonSwadesh{} non-Swadesh concepts drawn from culturally specific, modern, and abstract vocabulary (e.g., \textit{government}, \textit{university}, \textit{airport}, \textit{democracy}).

Counter to the na\"ive prediction that culturally universal concepts should converge more, the non-Swadesh vocabulary exhibits \textit{higher} mean convergence ($\mu = \NonSwadeshCompMean{}$) than Swadesh items ($\mu = \SwadeshCompMean{}$), with a Mann-Whitney $U = \SwadeshCompU{}$ and $p = \SwadeshCompP{}$ (Cohen's $d = \SwadeshCompCohenD{}$).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig_swadesh_comparison.pdf}
  \caption{Convergence distributions for Swadesh core vocabulary ($n = \SwadeshCompNumSwadesh{}$) and non-Swadesh modern vocabulary ($n = \SwadeshCompNumNonSwadesh{}$). Contrary to the cultural stability prediction, non-Swadesh concepts show higher mean convergence, driven by loanword orthographic similarity.}
  \label{fig:swadesh_comp}
\end{figure}

This reversal is informative rather than damaging to the broader argument.
Modern abstract vocabulary is disproportionately composed of loanwords: terms like \textit{democracy}, \textit{university}, and \textit{telephone} share surface forms across dozens of languages precisely because they were culturally borrowed rather than independently coined.
High embedding convergence for these items therefore reflects orthographic and phonological similarity---the model recognizes shared subword tokens---rather than deep semantic universality.
By contrast, Swadesh items denote universal concepts with \textit{diverse} surface forms across unrelated language families: the word for ``water'' differs radically between English, Mandarin, Arabic, and Swahili, yet must converge in the model's semantic space if conceptual structure is truly universal.
The moderate convergence of Swadesh items is thus the more remarkable finding, as it emerges despite surface-form diversity.
This distinction between borrowing-driven convergence and semantically-driven convergence motivates our remaining experiments, which use external benchmarks to validate that the Swadesh convergence signal reflects genuine conceptual structure.

\subsection{Variance Decomposition}

To quantify the surface-form contribution to convergence, we regress each concept's corrected convergence score against two predictors computed across Latin-script language pairs: (a) mean orthographic similarity (normalized Levenshtein distance on raw word forms) and (b) mean phonological similarity (Levenshtein distance after crude phonetic normalization---diacritic stripping, voiced/voiceless merging, geminate collapsing).
The orthographic predictor explains only $R^2 = \DecompRsqOrtho{}$ of the variance in convergence scores, and the phonological predictor explains even less ($R^2 = \DecompRsqPhon{}$).

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_variance_decomposition.pdf}
  \caption{Variance decomposition: corrected convergence score vs.\ (a) mean orthographic similarity and (b) mean phonological similarity for each Swadesh concept, colored by semantic category. Neither surface-form predictor explains more than 1.2\% of the variance in embedding convergence, confirming that the convergence signal is predominantly semantic rather than driven by cognate or borrowed forms.}
  \label{fig:variance_decomp}
\end{figure*}

Figure~\ref{fig:variance_decomp} shows that neither orthographic nor phonological similarity predicts embedding convergence.
Concepts with high convergence scores are not simply those with cognate or borrowed word forms: the signal is predominantly semantic.
The residual convergence---over 98\% of the variance---reflects the deeper conceptual structure that our subsequent experiments probe.

\subsection{Category Summary}

Grouping the \NumConcepts{} Swadesh items by semantic category reveals a clear hierarchy of convergence.
Nature terms (mean $= \CatNatureMean{} \pm \CatNatureStd{}$) and People terms (mean $= \CatPeopleMean{} \pm \CatPeopleStd{}$) converge most strongly, while Pronouns (mean $= \CatPronounsMean{} \pm \CatPronounsStd{}$) converge least.
Figure~\ref{fig:category_summary} presents this breakdown.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig_category_summary.pdf}
  \caption{Mean convergence score by Swadesh semantic category (corrected embeddings). Error bars show within-category standard deviation. Nature and People categories converge most strongly; Pronouns converge least, consistent with their high cross-linguistic grammaticalization variability.}
  \label{fig:category_summary}
\end{figure}

The category hierarchy aligns with linguistic intuitions: concrete, perceptually grounded categories (Nature, Animals) converge more than grammatical categories (Pronouns), which exhibit greater cross-linguistic variability in form and function.

\paragraph{Polysemy confound.}
Several low-scoring concepts---\textit{bark}, \textit{lie}, \textit{fly}---are systematically polysemous in English, where the Swadesh list was defined.
Because our carrier sentence does not disambiguate senses, the model may produce a blend representation that averages across senses available in the source language \citep{miller1995}.
Languages that lack the English polysemy (e.g., separate words for ``bark of a tree'' and ``a dog's bark'') will produce sense-specific embeddings that diverge from this blend, artificially depressing cross-lingual convergence.
This polysemy confound affects the bottom of the ranking more than the top, reinforcing the conclusion that high-convergence items genuinely reflect universal semantic structure.

\subsection{Isotropy Correction Validation}

Our ABTT isotropy correction rescales similarity values but largely preserves the relative ordering of concepts.
The Spearman rank correlation between raw and corrected convergence rankings is $\rho = \IsotropySpearmanRho{}$ ($p = \IsotropySpearmanP{}$), indicating near-perfect ordinal agreement.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_isotropy_validation.pdf}
  \caption{Isotropy correction validation. (a) Scatter of raw vs.\ corrected convergence scores (Spearman $\rho = \IsotropySpearmanRho{}$), colored by semantic category. Points below the diagonal indicate concepts whose convergence decreased after correction. (b) Side-by-side comparison of the top-20 concepts under each regime; the overlap is substantial but not total, with a few concepts (e.g., pronouns) reranked.}
  \label{fig:isotropy_validation}
\end{figure*}

Figure~\ref{fig:isotropy_validation} shows that the correction compresses the similarity scale (corrected values are generally lower) but preserves the overall ranking structure.
The few reranked concepts tend to be pronouns and function words whose raw convergence was inflated by the anisotropic bias toward high-frequency tokens.
This validation supports the use of corrected convergence scores throughout our analyses.

\subsection{Phylogenetic Distance Correlation}

To assess whether the embedding space preserves genealogical signal, we applied the Mantel test to the embedding distance matrix (averaged over all Swadesh concepts) and the ASJP phonetic distance matrix \citep{jaeger2018} across \MantelNumLangs{} languages for which both data sources are available.
The resulting correlation is $\rho = \MantelRho{}$ ($p = \MantelP{}$, \MantelPermutations{} permutations), indicating a statistically significant but modest association.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig_phylogenetic.pdf}
  \caption{Phylogenetic structure in the NLLB-200 embedding space. Left: heatmap of pairwise embedding distances between \MantelNumLangs{} languages, ordered by hierarchical clustering. Right: dendrogram derived from the embedding distance matrix. Major language families (e.g., Indo-European, Austronesian, Niger-Congo) form recognizable clusters.}
  \label{fig:phylo}
\end{figure}

Figure~\ref{fig:phylo} presents the hierarchical clustering derived from embedding distances.
Recognizable family-level groupings emerge: Indo-European languages cluster together, as do Austronesian, Turkic, and Niger-Congo languages.
However, the modest magnitude of $\rho$ indicates that genealogical relatedness explains only a fraction of the variance in embedding geometry.

Figure~\ref{fig:mantel_scatter} provides a direct visualization of the Mantel correlation by plotting each language pair's ASJP phonetic distance against its embedding distance.
The positive trend is visible but noisy, confirming that the correlation is real but weak.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig_mantel_scatter.pdf}
  \caption{Mantel test scatter: pairwise embedding distance vs.\ ASJP phonetic distance across \MantelNumLangs{} languages (subsampled to 2000 pairs for legibility). The positive trend ($\rho = \MantelRho{}$, $p = \MantelP{}$) indicates that more genetically distant languages occupy more distant embedding regions.}
  \label{fig:mantel_scatter}
\end{figure}

Complementing the language-level view, Figure~\ref{fig:concept_map} provides a concept-level perspective by projecting the cross-lingual centroids of all \NumConcepts{} Swadesh items into a 2D PCA space, colored by semantic category.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig_concept_map.pdf}
  \caption{2D PCA projection of cross-lingual concept centroids for all \NumConcepts{} Swadesh items, colored by semantic category. Concepts from the same semantic domain (e.g., body parts, nature terms) tend to cluster, revealing that the model's representation space is organized by meaning.}
  \label{fig:concept_map}
\end{figure}

The concept map reveals that semantically related items (e.g., body parts, nature terms) cluster together in the shared embedding space, providing visual evidence that the model's geometry is organized by conceptual content rather than arbitrary lexical associations.

The model's representations are shaped primarily by translational equivalence rather than surface-level phonological or morphological similarity, which explains the incomplete correspondence with phonetic distance.
This is consistent with the view that NLLB-200's shared encoder constructs a representation space organized predominantly around meaning, with historical signal as a secondary structuring force.

\subsection{Colexification Proximity}

We tested whether concept pairs that are colexified in the CLICS\textsuperscript{3} database \citep{list2018,rzymski2020}---that is, expressed by the same word form in at least one language---are represented more similarly than non-colexified pairs in the NLLB-200 encoder space.
Colexified concept pairs exhibit a mean cosine similarity of \ColexColMean{}, compared to \ColexNonColMean{} for non-colexified pairs.
A Mann-Whitney $U$ test confirms that this difference is statistically significant ($U = \ColexU{}$, $p = \ColexP{}$), with a medium effect size (Cohen's $d = \ColexCohenD{}$).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig_colexification.pdf}
  \caption{Cosine similarity distributions for colexified ($n = \ColexColCount{}$) and non-colexified ($n = \ColexNonColCount{}$) concept pairs. Colexified pairs are significantly more similar in the NLLB-200 embedding space, suggesting the model has internalized cross-linguistically recurrent semantic associations.}
  \label{fig:colex}
\end{figure}

The medium effect size reported in Figure~\ref{fig:colex} indicates that the model has internalized universal conceptual associations that transcend individual languages.
Colexification patterns arise from shared cognitive and experiential structure across human populations \citep{list2018}, and the fact that a translation model trained without explicit semantic annotation recovers these associations provides evidence that cross-lingual translational equivalence is sufficient to induce conceptually meaningful geometric structure.

\subsection{Conceptual Store Metric}

To quantify the degree to which NLLB-200's representation space is organized by concept rather than by language, we compute the ratio of mean between-concept cosine distance to mean within-concept cosine distance.
On raw embeddings, this ratio is \ConceptualStoreRaw{}, indicating that even before correction, translation-equivalent words are closer to each other than to words denoting different concepts.

After per-language mean-centering---which removes each language's systematic offset in the shared space---the ratio increases to \ConceptualStoreCentered{}, an improvement factor of \ConceptualStoreImprovement{}$\times$.
This improvement confirms that a substantial component of the raw embedding geometry reflects language identity rather than semantics, and that subtracting language centroids exposes a cleaner conceptual structure.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig_conceptual_store.pdf}
  \caption{Conceptual store metric before and after per-language mean-centering. The between-concept to within-concept distance ratio increases from \ConceptualStoreRaw{} to \ConceptualStoreCentered{} after correction, revealing a latent conceptual organization that is partially masked by language-level clustering in the raw space.}
  \label{fig:conceptual_store}
\end{figure}

The result depicted in Figure~\ref{fig:conceptual_store} resonates with neuroscientific findings of language-independent conceptual stores in anterior temporal cortex \citep{correia2014}.
Just as bilingual speakers access shared semantic representations across their languages, NLLB-200's encoder appears to construct a representational substrate where meaning is partially factored from language identity---a property that emerges from the translational training objective without explicit encouragement.

\subsection{Color Circle}

We project the cross-lingual centroids of the \ColorNumColors{} basic color terms identified by \citet{berlin1969} into a two-dimensional PCA space using embeddings from \ColorNumLanguages{} languages.
Figure~\ref{fig:color} shows the resulting arrangement.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig_color_circle.pdf}
  \caption{PCA projection of cross-lingual centroids for \ColorNumColors{} Berlin \& Kay basic color terms across \ColorNumLanguages{} languages. Warm colors (red, orange, yellow) and cool colors (blue, green) occupy opposing regions, and the overall arrangement recovers an approximate circular topology consistent with perceptual color space.}
  \label{fig:color}
\end{figure}

The projection reveals a striking arrangement: warm colors (red, orange, yellow) and cool colors (blue, green) occupy opposing regions of the plane, and adjacent colors in perceptual space (e.g., red--orange, blue--green) are adjacent in the PCA projection.
The overall layout approximates the circular topology of perceptual color wheels, despite the model never having received explicit perceptual training.
This finding suggests that the co-occurrence and translation statistics across \ColorNumLanguages{} languages implicitly encode perceptual similarity---languages that partition the color spectrum differently nonetheless exert a collective pressure that shapes the encoder's geometry toward a perceptually coherent arrangement.
White, black, and grey occupy positions partially separated from the chromatic circle, consistent with their distinct status in the Berlin and Kay hierarchy.

\subsection{Semantic Offset Invariance}

We evaluate whether semantic relationships are encoded as consistent vector offsets across languages by examining \OffsetNumPairs{} concept pairs.
For each pair, we compute the offset vector in each language and measure its cosine similarity to the centroid offset (averaged over all languages).
The mean cross-lingual consistency across all pairs is \OffsetMeanConsistency{}, with individual pairs ranging from \OffsetMinConsistency{} to \OffsetMaxConsistency{}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig_offset_invariance.pdf}
  \caption{Semantic offset invariance across languages. Each bar shows the mean cosine similarity between per-language offset vectors and the centroid offset for a given concept pair. The best-performing pair is \textit{\OffsetBestPair{}}.}
  \label{fig:offset}
\end{figure}

The best-performing pair is \textit{\OffsetBestPair{}}, achieving a consistency score of \OffsetMaxConsistency{}.
As shown in Figure~\ref{fig:offset}, the high overall consistency (mean = \OffsetMeanConsistency{}) indicates that the directional relationships between concepts are largely preserved across languages in the shared encoder space.
This extends the classical word2vec analogy finding \citep{mikolov2013} to a massively multilingual setting: not only do semantic offsets exist within a single language's embedding space, but they are approximately invariant across \NumLanguages{} typologically diverse languages.
The result provides evidence for a shared relational geometry in the NLLB-200 encoder that goes beyond point-wise translational equivalence to encode structured semantic relationships in a language-general manner \citep{chang2022}.

The variation across pairs is itself informative.
Pairs involving concrete, perceptually grounded oppositions (e.g., \textit{\OffsetBestPair{}}) tend to exhibit higher consistency than those involving more abstract or culturally variable relationships.
This gradient mirrors the convergence hierarchy observed in the Swadesh ranking (Section~\ref{sec:results}), reinforcing the conclusion that NLLB-200's cross-lingual alignment is strongest for meanings that are universally experienced and least ambiguous.

Figure~\ref{fig:offset_family_heatmap} disaggregates offset consistency by language family, revealing that Indo-European and Turkic families tend to show high consistency across most concept pairs, while more typologically distant families (e.g., Niger-Congo, Tai-Kadai) show greater variability.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_offset_family_heatmap.pdf}
  \caption{Per-family offset consistency heatmap. Each cell shows the mean cosine similarity between per-language offset vectors and the centroid offset, averaged over languages within each family. Rows are concept pairs (sorted by overall consistency); columns are language families. Warmer colors indicate higher consistency.}
  \label{fig:offset_family_heatmap}
\end{figure*}

Finally, Figure~\ref{fig:offset_vector_demo} provides a geometric illustration of the best-performing pair (\textit{\OffsetBestPair{}}), showing that per-language offset vectors align closely with the centroid direction.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_offset_vector_demo.pdf}
  \caption{Offset vector demonstration for the best-performing pair (\textit{\OffsetBestPair{}}). (a) 2D PCA projection of per-language embeddings for both concepts, with centroids marked by stars. (b) Offset vectors from origin, showing that per-language vectors (colored by family) align closely with the centroid offset (black arrow), indicating language-invariant relational structure.}
  \label{fig:offset_vector_demo}
\end{figure*}
