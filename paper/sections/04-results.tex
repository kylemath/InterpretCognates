\section{Results}
\label{sec:results}

We present our six core experiments alongside descriptive illustrations and validation analyses, organized along a progression from broad distributional patterns through external validation to geometric tests of cognitive hypotheses.

\subsection{Illustrative Example: Water}

Before presenting the full ranking, we illustrate the geometry of a single concept.
Figure~\ref{fig:water_manifold} shows the 29-language embedding manifold for ``water'' --- a concept with diverse surface forms across language families (\textit{agua}, \textit{eau}, \textit{Wasser}, \textit{maji}, etc.).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_water_manifold.pdf}
  \caption{Embedding geometry for the concept ``water'' across 29 languages. (a) 3D PCA projection colored by language family shows tight clustering despite orthographic diversity. (b) Pairwise similarity heatmap reveals that same-family languages (e.g., Romance, Slavic) cluster, but cross-family similarity remains high ($>0.93$ for most pairs).}
  \label{fig:water_manifold}
\end{figure}

Despite radically different surface forms and scripts, the embeddings cluster tightly in representation space: same-family languages form sub-clusters, but cross-family similarity remains high, suggesting that the model maps ``water'' to a shared semantic region regardless of its lexical realization.
This example motivates the systematic analysis that follows.

\subsection{Swadesh Core Vocabulary Convergence}

Across the \NumConcepts{} Swadesh items embedded in \NumLanguages{} languages, the mean cross-lingual convergence score---defined as the average pairwise cosine similarity over all language pairs for a given concept---is \SwadeshMean{} ($\sigma = \SwadeshStd{}$), with individual concepts ranging from \SwadeshMin{} to \SwadeshMax{}.
Figure~\ref{fig:swadesh} presents the full ranking.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_swadesh_ranking.pdf}
  \caption{Swadesh convergence ranking. Each concept is plotted by mean cross-lingual phonetic similarity among Latin-script translations (x-axis) versus embedding convergence in NLLB-200's encoder (y-axis, isotropy-corrected). Points are colored by semantic category. Concepts in the upper-left region converge strongly in embedding space \emph{despite} low surface-form similarity---the strongest candidates for genuine conceptual universals.}
  \label{fig:swadesh}
\end{figure*}

The highest-ranked concept is \textit{\SwadeshTopConcept{}}, while the lowest is \textit{\SwadeshBottomConcept{}}.
The distribution reveals a clear pattern: concepts that are concrete, perceptually grounded, and monosemous (e.g., body parts, celestial objects, kinship terms) tend to cluster near the top, whereas concepts that are abstract or polysemous tend to occupy the bottom ranks.
Several of the lowest-scoring items---such as \textit{bark} (tree covering vs.\ the sound a dog makes) and \textit{lie} (recline vs.\ falsehood)---are well-known cases of systematic polysemy in English that do not transfer to other languages, resulting in dispersed cross-lingual representations.
This ordering is broadly consistent with the intuition behind the Swadesh list itself: the most culturally stable meanings \citep{swadesh1952} are also those that the model encodes most uniformly.

\subsection{Swadesh vs.\ Non-Swadesh Vocabulary}

As a sanity check, we compared convergence scores for \SwadeshCompNumSwadesh{} Swadesh concepts against \SwadeshCompNumNonSwadesh{} non-Swadesh concepts drawn from modern and institutional vocabulary (e.g., \textit{telephone}, \textit{university}, \textit{democracy}, \textit{restaurant}).
Counter to the na\"ive prediction that culturally universal concepts should converge more, the non-Swadesh set exhibits \textit{higher} mean convergence ($\mu = \NonSwadeshCompMean{}$ vs.\ $\mu = \SwadeshCompMean{}$; Mann-Whitney $U = \SwadeshCompU{}$, $p = \SwadeshCompP{}$, Cohen's $d = \SwadeshCompCohenD{}$).

This reversal is largely an artifact of our non-Swadesh selection, which is heavily biased toward loanwords.
Terms like \textit{democracy}, \textit{telephone}, \textit{computer}, and \textit{hotel} share surface forms across dozens of languages because they were culturally borrowed rather than independently coined; the model's high convergence for these items therefore reflects shared subword tokens rather than deep semantic universality.
By contrast, Swadesh items denote universal concepts with \textit{diverse} surface forms across unrelated language families: the word for ``water'' differs radically between English, Mandarin, Arabic, and Swahili, yet must converge in the model's semantic space if conceptual structure is truly universal.
The moderate convergence of Swadesh items is thus the more noteworthy finding, as it emerges despite surface-form diversity.
A properly controlled comparison would require a non-Swadesh set matched for borrowability and orthographic diversity; we leave this to future work.
This distinction between borrowing-driven convergence and semantically-driven convergence motivates the variance decomposition analysis that follows, which directly quantifies the surface-form contribution.

\subsection{Variance Decomposition}

To quantify the surface-form contribution to convergence, we regress each concept's corrected convergence score against two predictors computed across Latin-script language pairs: (a) mean orthographic similarity (normalized Levenshtein distance on raw word forms) and (b) mean phonological similarity (Levenshtein distance after crude phonetic normalization---diacritic stripping, voiced/voiceless merging, geminate collapsing).
The orthographic predictor explains only $R^2 = \DecompRsqOrtho{}$ of the variance in convergence scores, and the phonological predictor explains even less ($R^2 = \DecompRsqPhon{}$).

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_variance_decomposition.pdf}
  \caption{Variance decomposition: corrected convergence score vs.\ (a) mean orthographic similarity and (b) mean phonological similarity for each Swadesh concept, colored by semantic category. Neither surface-form predictor explains more than 1.2\% of the variance in embedding convergence, confirming that the convergence signal is predominantly semantic rather than driven by cognate or borrowed forms.}
  \label{fig:variance_decomp}
\end{figure*}

Figure~\ref{fig:variance_decomp} shows that neither orthographic nor phonological similarity predicts embedding convergence.
Concepts with high convergence scores are not simply those with cognate or borrowed word forms: the signal is predominantly semantic.
The residual convergence---over 98\% of the variance---reflects the deeper conceptual structure that our subsequent experiments probe.

\subsection{Category Summary}

Grouping the \NumConcepts{} Swadesh items by semantic category reveals a clear hierarchy of convergence.
Nature terms (mean $= \CatNatureMean{} \pm \CatNatureStd{}$) and People terms (mean $= \CatPeopleMean{} \pm \CatPeopleStd{}$) converge most strongly, while Pronouns (mean $= \CatPronounsMean{} \pm \CatPronounsStd{}$) converge least.
Figure~\ref{fig:category_summary} presents this breakdown.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_category_summary.pdf}
  \caption{Convergence by semantic category. (a) Box-and-whisker plots and (b) violin plots with individual data points for each Swadesh semantic category (isotropy-corrected). Dashed lines mark the overall mean. Nature and People categories converge most strongly; Pronouns converge least, consistent with their high cross-linguistic grammaticalization variability.}
  \label{fig:category_summary}
\end{figure*}

The category hierarchy aligns with linguistic intuitions: concrete, perceptually grounded categories (Nature, Animals) converge more than grammatical categories (Pronouns), which exhibit greater cross-linguistic variability in form and function.

\paragraph{Polysemy confound.}
Several low-scoring concepts---\textit{bark}, \textit{lie}, \textit{fly}---are systematically polysemous in English, where the Swadesh list was defined.
Because our carrier sentence does not disambiguate senses, the model may produce a blend representation that averages across senses available in the source language \citep{miller1995}.
Languages that lack the English polysemy (e.g., separate words for ``bark of a tree'' and ``a dog's bark'') will produce sense-specific embeddings that diverge from this blend, artificially depressing cross-lingual convergence.
This polysemy confound affects the bottom of the ranking more than the top, reinforcing the conclusion that high-convergence items genuinely reflect universal semantic structure.

\subsection{Isotropy Correction Validation}

Our ABTT isotropy correction rescales similarity values but largely preserves the relative ordering of concepts.
The Spearman rank correlation between raw and corrected convergence rankings is $\rho = \IsotropySpearmanRho{}$ ($p = \IsotropySpearmanP{}$), indicating near-perfect ordinal agreement.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_isotropy_validation.pdf}
  \caption{Isotropy correction validation. (a) Scatter of raw vs.\ corrected convergence scores (Spearman $\rho = \IsotropySpearmanRho{}$), colored by semantic category. Points below the diagonal indicate concepts whose convergence decreased after correction. (b) Side-by-side comparison of the top-20 concepts under each regime; the overlap is substantial but not total, with a few concepts (e.g., pronouns) reranked.}
  \label{fig:isotropy_validation}
\end{figure*}

Figure~\ref{fig:isotropy_validation} shows that the correction compresses the similarity scale (corrected values are generally lower) but preserves the overall ranking structure.
The few reranked concepts tend to be pronouns and function words whose raw convergence was inflated by the anisotropic bias toward high-frequency tokens.
This validation supports the use of corrected convergence scores throughout our analyses.

\subsection{Phylogenetic Distance Correlation}

To assess whether the embedding space preserves genealogical signal, we applied the Mantel test to the embedding distance matrix (averaged over all Swadesh concepts) and the ASJP phonetic distance matrix \citep{jaeger2018} across \MantelNumLangs{} languages for which both data sources are available.
The resulting correlation is $\rho = \MantelRho{}$ ($p = \MantelP{}$, \MantelPermutations{} permutations), indicating a statistically significant but modest association.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig_phylogenetic.pdf}
  \caption{Phylogenetic structure in the NLLB-200 embedding space. Left: heatmap of pairwise embedding distances between \MantelNumLangs{} languages, ordered by hierarchical clustering. Right: dendrogram derived from the embedding distance matrix. Major language families (e.g., Indo-European, Austronesian, Niger-Congo) form recognizable clusters.}
  \label{fig:phylo}
\end{figure}

Figure~\ref{fig:phylo} presents the hierarchical clustering derived from embedding distances.
Recognizable family-level groupings emerge: Indo-European languages cluster together, as do Austronesian, Turkic, and Niger-Congo languages.
However, the modest magnitude of $\rho$ indicates that genealogical relatedness explains only a fraction of the variance in embedding geometry.

Figure~\ref{fig:mantel_scatter} provides a direct visualization of the Mantel correlation by plotting each language pair's ASJP phonetic distance against its embedding distance, stratified by phylogenetic relationship.
Same-subfamily pairs (e.g., French--Spanish) cluster at low ASJP distance with tight embedding distances; cross-branch Indo-European pairs (e.g., English--Russian) occupy the middle range; and cross-family pairs (e.g., English--Chinese) dominate the high-distance region.
Separate per-group regression lines reveal that the positive trend is driven primarily by the contrast between these tiers rather than a uniform linear relationship.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig_mantel_scatter.pdf}
  \caption{Mantel test scatter: pairwise embedding distance vs.\ ASJP phonetic distance across \MantelNumLangs{} languages, colored by phylogenetic relationship tier---same subfamily (blue), cross-branch Indo-European (amber), and cross-family (red). Dashed lines show per-group linear fits with Spearman $\rho$ values. The overall Mantel $\rho = \MantelRho{}$ ($p = \MantelP{}$).}
  \label{fig:mantel_scatter}
\end{figure}

Complementing the language-level view, Figure~\ref{fig:concept_map} provides a concept-level perspective by projecting the cross-lingual centroids of all \NumConcepts{} Swadesh items into a 2D PCA space, colored by semantic category.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_concept_map.pdf}
  \caption{2D PCA projection of Swadesh concept embeddings pooled across all 19 language families. Small translucent dots show per-family centroid positions (one per concept per family, ${\sim}1{,}900$ points total); convex hulls delineate each semantic category's spread; large opaque dots mark the overall cross-lingual centroids. Body parts and nature terms occupy distinct regions of the space, while pronouns cluster tightly---confirming that the model's geometry is organized by meaning rather than arbitrary lexical associations.}
  \label{fig:concept_map}
\end{figure*}

The concept map reveals that semantically related items (e.g., body parts, nature terms) cluster together in the shared embedding space, providing visual evidence that the model's geometry is organized by conceptual content rather than arbitrary lexical associations.

The model's representations are shaped primarily by translational equivalence rather than surface-level phonological or morphological similarity, which explains the incomplete correspondence with phonetic distance.
This is consistent with the view that NLLB-200's shared encoder constructs a representation space organized predominantly around meaning, with historical signal as a secondary structuring force.

\subsection{Colexification Proximity}

We assessed whether colexification frequency---the number of language families in CLICS\textsuperscript{3} \citep{list2018,rzymski2020} that express two concepts with the same word form---predicts embedding similarity in the NLLB-200 encoder space.
Treating colexification as a continuous variable across all \ColexNumPairs{} Swadesh concept pairs yields a significant positive Spearman correlation ($\rho_s = \ColexSpearmanRho{}$, $p = \ColexSpearmanP{}$): the more language families that colexify a pair, the more similar the model's representations.
A confirmatory Mann-Whitney $U$ test on the binary split (colexified $\geq 3$ families vs.\ non-colexified) corroborates this gradient ($U = \ColexU{}$, $p = \ColexP{}$, Cohen's $d = \ColexCohenD{}$).

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_colexification.pdf}
  \caption{Cosine similarity as a function of colexification frequency for \ColexNumPairs{} Swadesh concept pairs. Each red point is a pair attested by at least one CLICS\textsuperscript{3} language family; grey points are non-colexified controls. The dashed line is a linear fit. Spearman $\rho_s = \ColexSpearmanRho{}$ ($p = \ColexSpearmanP{}$). Selected pairs are labelled to illustrate the semantic content at different frequency levels.}
  \label{fig:colex}
\end{figure*}

The continuous relationship visible in Figure~\ref{fig:colex} indicates that the model's geometry tracks the \emph{strength} of cross-linguistic semantic association, not merely its presence or absence.
Colexification patterns arise from shared cognitive and experiential structure across human populations \citep{list2018}, and the monotonic increase in embedding similarity with colexification frequency provides evidence that NLLB-200's shared encoder has internalized a graded scale of conceptual relatedness through translational equivalence alone.

\subsection{Conceptual Store Metric}

To quantify the degree to which NLLB-200's representation space is organized by concept rather than by language, we compute the ratio of mean between-concept cosine distance to mean within-concept cosine distance.
On raw embeddings, this ratio is \ConceptualStoreRaw{}, indicating that even before correction, translation-equivalent words are closer to each other than to words denoting different concepts.

After per-language mean-centering---which removes each language's systematic offset in the shared space---the ratio increases to \ConceptualStoreCentered{}, an improvement factor of \ConceptualStoreImprovement{}$\times$.
This improvement confirms that a substantial component of the raw embedding geometry reflects language identity rather than semantics, and that subtracting language centroids exposes a cleaner conceptual structure.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig_conceptual_store.pdf}
  \caption{Conceptual store metric before and after per-language mean-centering. The between-concept to within-concept distance ratio increases from \ConceptualStoreRaw{} to \ConceptualStoreCentered{} after correction. Error bars show 95\% bootstrap confidence intervals (resampling over concepts). The improvement reveals a latent conceptual organization partially masked by language-level clustering in the raw space.}
  \label{fig:conceptual_store}
\end{figure}

The result depicted in Figure~\ref{fig:conceptual_store} resonates with neuroscientific findings of language-independent conceptual stores in anterior temporal cortex \citep{correia2014}.
Just as bilingual speakers access shared semantic representations across their languages, NLLB-200's encoder appears to construct a representational substrate where meaning is partially factored from language identity---a property that emerges from the translational training objective without explicit encouragement.

\subsection{Color Circle}

We project the cross-lingual centroids of the \ColorNumColors{} basic color terms identified by \citet{berlin1969} into a two-dimensional PCA space using embeddings from \ColorNumLanguages{} languages.
Figure~\ref{fig:color} shows the resulting arrangement.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_color_circle.pdf}
  \caption{PCA projection of \ColorNumColors{} Berlin \& Kay basic color terms across \ColorNumLanguages{} languages. Small translucent dots show individual per-language embeddings; convex hulls delineate the spread of each color term across languages; large filled circles mark the cross-lingual centroids. Warm colors (red, orange, yellow) and cool colors (blue, green) occupy opposing regions, and the overall arrangement recovers an approximate circular topology consistent with perceptual color space.}
  \label{fig:color}
\end{figure*}

The projection reveals a striking arrangement: warm colors (red, orange, yellow) and cool colors (blue, green) occupy opposing regions of the plane, and adjacent colors in perceptual space (e.g., red--orange, blue--green) are adjacent in the PCA projection.
The overall layout approximates the circular topology of perceptual color wheels, despite the model never having received explicit perceptual training.
This finding suggests that the co-occurrence and translation statistics across \ColorNumLanguages{} languages implicitly encode perceptual similarity---languages that partition the color spectrum differently nonetheless exert a collective pressure that shapes the encoder's geometry toward a perceptually coherent arrangement.
White, black, and grey occupy positions partially separated from the chromatic circle, consistent with their distinct status in the Berlin and Kay hierarchy.

\subsection{Semantic Offset Invariance}

We evaluate whether semantic relationships are encoded as consistent vector offsets across languages by examining \OffsetNumPairs{} concept pairs.
For each pair, we compute the offset vector in each language and measure its cosine similarity to the centroid offset (averaged over all languages).
The mean cross-lingual consistency across all pairs is \OffsetMeanConsistency{}, with individual pairs ranging from \OffsetMinConsistency{} to \OffsetMaxConsistency{}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig_offset_invariance.pdf}
  \caption{Semantic offset invariance across languages. Each bar shows the mean cosine similarity between per-language offset vectors and the centroid offset for a given concept pair. The best-performing pair is \textit{\OffsetBestPair{}}.}
  \label{fig:offset}
\end{figure}

The best-performing pair is \textit{\OffsetBestPair{}}, achieving a consistency score of \OffsetMaxConsistency{}.
As shown in Figure~\ref{fig:offset}, the high overall consistency (mean = \OffsetMeanConsistency{}) indicates that the directional relationships between concepts are largely preserved across languages in the shared encoder space.
This extends the classical word2vec analogy finding \citep{mikolov2013} to a massively multilingual setting: not only do semantic offsets exist within a single language's embedding space, but they are approximately invariant across \NumLanguages{} typologically diverse languages.
The result provides evidence for a shared relational geometry in the NLLB-200 encoder that goes beyond point-wise translational equivalence to encode structured semantic relationships in a language-general manner \citep{chang2022}.

The variation across pairs is itself informative.
Pairs involving concrete, perceptually grounded oppositions (e.g., \textit{\OffsetBestPair{}}) tend to exhibit higher consistency than those involving more abstract or culturally variable relationships.
This gradient mirrors the convergence hierarchy observed in the Swadesh ranking (Section~\ref{sec:results}), reinforcing the conclusion that NLLB-200's cross-lingual alignment is strongest for meanings that are universally experienced and least ambiguous.

Figure~\ref{fig:offset_family_heatmap} disaggregates offset consistency by language family, revealing that Indo-European and Turkic families tend to show high consistency across most concept pairs, while more typologically distant families (e.g., Niger-Congo, Tai-Kadai) show greater variability.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_offset_family_heatmap.pdf}
  \caption{Per-family offset consistency heatmap. Each cell shows the mean cosine similarity between per-language offset vectors and the centroid offset, averaged over languages within each family. Rows are concept pairs (sorted by overall consistency); columns are language families. Warmer colors indicate higher consistency.}
  \label{fig:offset_family_heatmap}
\end{figure*}

Finally, Figure~\ref{fig:offset_vector_demo} provides a geometric illustration of the top four concept pairs, visualizing how per-language offset vectors align with the centroid direction across the shared PCA space.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_offset_vector_demo.pdf}
  \caption{Offset vector demonstration for the top-4 concept pairs. (a) Joint PCA projection showing per-language embeddings (translucent), centroid positions (white markers), and offset arrows for each pair (colored). Thin arrows show individual per-language offsets; bold arrows show centroid offsets. (b) All offset vectors plotted from a common origin, revealing directional consistency: per-language offsets cluster tightly around their centroid direction for each pair, confirming language-invariant relational structure.}
  \label{fig:offset_vector_demo}
\end{figure*}
